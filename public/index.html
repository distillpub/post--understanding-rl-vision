<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=1200">
  <script src="https://distill.pub/template.v2.js"></script>
  <style>
    .subgrid {
  grid-column: screen; 
  display: grid; 
  grid-template-columns: inherit;
  grid-template-rows: inherit;
  grid-column-gap: inherit;
  grid-row-gap: inherit;
}

d-figure.base-grid {
  grid-column: screen;
  background: hsl(0, 0%, 97%);
  padding: 20px 0;
  border-top: 1px solid rgba(0, 0, 0, 0.1);
  border-bottom: 1px solid rgba(0, 0, 0, 0.1);
}

d-figure {
  margin-bottom: 1em;
  position: relative;
}

d-figure > figure {
  margin-top: 0;
  margin-bottom: 0;
}

.shaded-figure {
  background-color: hsl(0, 0%, 97%);
  border-top: 1px solid hsla(0, 0%, 0%, 0.1);
  border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
  padding: 30px 0;
}

.pointer {
  position: absolute;
  width: 26px;
  height: 26px;
  top: 26px;
  left: -48px;
}

.grayscale-light {
  filter: gray;
  -webkit-filter: grayscale(1) contrast(0.2) brightness(1.6);
  filter: grayscale(1) contrast(0.2) brightness(1.6);
}

.grayscale-dark {
  filter: gray;
  -webkit-filter: grayscale(1) brightness(0.75);
  filter: grayscale(1) brightness(0.75);
}

.striped {
  background: repeating-linear-gradient(135deg, lightgray 0px, whitesmoke 10px, lightgray 20px);
}

body d-title {
  overflow-x: auto;
}

body d-article {
  overflow: visible;
}

d-title h1, d-title p, d-title figure {
  grid-column: page;
}

b {
  font-weight: bold;
}

label, button {
  cursor: pointer;
}

button {
  border-radius: 0.25em;
}

#coinrun-objects {
  grid-column: page;
}

#coinrun-objects .coinrun-objects-row-images td {
  white-space: nowrap;
}

#coinrun-objects img {
  width: auto;
  border: 1px solid gray;
}

#coinrun-objects .coinrun-object-single {
  width: 100%;
}

#coinrun-objects .coinrun-object-double {
  width: 48%;
}

#coinrun-objects .coinrun-objects-row-text td {
  position: relative;
  height: 9em;
}

#coinrun-objects .coinrun-objects-row-text figcaption {
  position: absolute;
  top: 1em;
}

#coinrun-actions .coinrun-action {
  font-weight: bold;
  padding: 0.2em;
  border: 1px solid gray;
  border-radius: 0.25em;
}

.play-pause-button {
  height: 1.3em;
  width: 1.3em;
  font-size: 3em;
  line-height: 0em;
}

.interface-failure-step-button {
  font-size: 1em;
  margin: 0em 0.1em;
}

#interface-failure-position {
  margin: 0em 1em;
}

.matplotlib-svg text, .matplotlib-svg tspan {
  font-family: inherit !important;
}

#model-editing-levels label {
  white-space: nowrap;
}

#model-editing-levels video {
  width: 100%;
}

#feature-vis-traditional td {
  padding: 8px;
}

#feature-vis-traditional img {
  display: inline-block;
  width: 128px;
  border: 1px solid gray;
}

#feature-vis-dataset {
  margin: 0 auto;
  text-align: center;
  max-width: 640px;
}

.feature-vis-dataset-item {
  display: inline-block;
  vertical-align: top;
  width: 136px;
}

#feature-vis-dataset img {
  display: block;
  width: 128px;
  border: 1px solid gray;
}

.feature-vis-dataset-text {
  margin-top: 0.2em;
  margin-bottom: 0.5em;
  font-size: 0.75em;
  line-height: 1.5em;
  font-style: italic;
}

#feature-vis-spatial {
  margin: 0.5em auto;
  height: 512px;
  width: 512px;
}

#feature-vis-spatial img {
  border: 1px solid gray;
}

#hero th, #hero td {
  vertical-align: top;
}

#hero #hero-annotations {
  position: relative;
  margin-top: 0.5em;
  font-size: 0.8em;
  color: gray;
}

#hero #hero-annotations > div {
  position: absolute;
  text-align: center;
  line-height: 1.1em;
  width: 100px;
}

#hero .hero-annotation-dot {
  display: inline-block;
  height: 16px;
  width: 16px;
  border-radius: 20%;
  vertical-align: top;
}

#hero .hero-annotation-image {
  height: auto;
  width: auto;
  margin: 1px;
  border: 1px solid gray;
  border-radius: 50%;
  cursor: pointer;
}

#hero .hero-annotation-line-vertical-outer {
  position: absolute;
  top: -142px;
  left: 47px;
  width: 6px;
  height: 137px;
  background-color: white;
}

#hero .hero-annotation-line-vertical-inner {
  position: absolute;
  z-index: 2;
  left: 2px;
  height: 143px;
  width: 2px;
  background-color: black;
}

#hero .hero-annotation-line-horizontal-outer {
  position: absolute;
  top: -144px;
  height: 6px;
  background-color: white;
}

#hero .hero-annotation-line-horizontal-inner {
  position: absolute;
  z-index: 2;
  top: 2px;
  height: 2px;
  background-color: black;
}

#attribution-demo td {
  width: 33.33%;
}

#attribution-demo td {
  min-width: 256px;
  max-width: 288px;
}

#hero td {
  min-width: 256px;
  max-width: 320px;
  padding: 2px 16px 2px 0px;
}

#attribution-demo .attribution-outer, #hero .hero-outer {
  position: relative;
  border: 1px solid gray;
  height: 0px;
  padding-bottom: 100%;
  width: 100%;
}

#attribution-demo .attribution-inner, #hero .hero-inner {
  position: absolute;
  height: 0px;
  padding-bottom: 100%;
  width: 100%;
}

#attribution-demo .attribution-image, #hero .hero-image {
  height: 0px;
  padding-bottom: 100%;
  width: 100%;
  background-size: 100% 100%;
}

#attribution-demo .attribution-legend-item {
  display: inline-block;
  padding: 0.5em;
  background: white;
  border: 1px solid white;
  border-radius: 0.25em;
  cursor: pointer;
  overflow: hidden;
}

#attribution-demo .attribution-legend-item:hover {
  background: whitesmoke;
  border: 1px solid gray;
}

#attribution-demo .attribution-legend-outer {
  position: relative;
  height: 192px;
  width: 128px;
}

#attribution-demo .attribution-legend-dot {
  position: absolute;
  top: 140.8px;
  left: 0px;
  height: 32px;
  width: 32px;
  border-radius: 20%;
}

#attribution-demo .attribution-legend-inner {
  position: absolute;
  height: 128px;
  width: 128px;
  top: 0px;
  left: 0px;
  border: 1px solid gray;
}

#attribution-demo .attribution-legend-label {
  position: absolute;
  height: 64px;
  width: 96px;
  top: 140.8px;
  left: 42.67px;
  font-size: 0.9em;
  line-height: 1.2em;
  font-style: italic;
  z-index: 1;
  text-align: left;
}

#coinrun-objects img, #feature-vis-traditional img, #feature-vis-dataset img, #feature-vis-spatial img, #attribution-demo .attribution-image, #hero .hero-image {
   image-rendering: optimizeSpeed;
   image-rendering: -moz-crisp-edges;
   image-rendering: -o-crisp-edges;
   image-rendering: -webkit-optimize-contrast;
   image-rendering: optimize-contrast;
   image-rendering: crisp-edges;
   image-rendering: pixelated;
   -ms-interpolation-mode: nearest-neighbor;
}

.architecture-list {
  margin-top: 0em;
}

.architecture-list li {
  margin-bottom: 0em;
}

/* table of contents */

@media (max-width: 1000px) {
  d-contents {
    justify-self: start;
    align-self: start;
    grid-column-start: 2;
    grid-column-end: 6;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom-width: 1px;
    border-bottom-style: solid;
    border-bottom-color: rgba(0, 0, 0, 0.1);
  }
}

@media (min-width: 1000px) {
  d-contents {
    align-self: start;
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: end;
    padding-right: 3em;
    padding-left: 2em;
    border-right: 1px solid rgba(0, 0, 0, 0.1);
    border-right-width: 1px;
    border-right-style: solid;
    border-right-color: rgba(0, 0, 0, 0.1);
  }
}

@media (min-width: 1180px) {
  d-contents {
    grid-column-start: 1;
    grid-column-end: 4;
    justify-self: end;
    padding-right: 3em;
    padding-left: 2em;
    border-right: 1px solid rgba(0, 0, 0, 0.1);
    border-right-width: 1px;
    border-right-style: solid;
    border-right-color: rgba(0, 0, 0, 0.1);
  }
}

d-contents nav h3 {
  margin-top: 0;
  margin-bottom: 1em;
}

d-contents nav a {
  color: rgba(0, 0, 0, 0.8);
  border-bottom: none;
  text-decoration: none;
}

d-contents li {
  list-style-type: none;
}

d-contents ul {
  padding-left: 1em;
}

d-contents nav ul li {
  margin-bottom: 0.25em;
}

d-contents nav a:hover {
  text-decoration: underline solid rgba(0, 0, 0, 0.6);
}

d-contents nav ul {
  margin-top: 0;
  margin-bottom: 6px;
}

d-contents nav > div {
  display: block;
  outline: none;
  margin-bottom: 0.5em;
}

d-contents nav > div > a {
  font-size: 13px;
  font-weight: 600;
}

d-contents nav > div > a:hover, d-contents nav > ul > li > a:hover {
  text-decoration: none;
}

  </style>
</head>

<body>

  <d-front-matter>
    <script type="text/json">{
      "title": "Understanding RL Vision",
      "description": "With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.",
      "authors": [
        {
          "author": "Jacob Hilton",
          "authorURL": "https://www.jacobh.co.uk/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        },
        {
          "author": "Nick Cammarata",
          "authorURL": "http://nickcammarata.com/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        },
        {
          "author": "Shan Carter",
          "authorURL": "http://shancarter.com/",
          "affiliation": "Observable",
          "affiliationURL": "http://observablehq.com/"
        },
        {
          "author": "Gabriel Goh",
          "authorURL": "http://gabgoh.github.io/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        },
        {
          "author": "Chris Olah",
          "authorURL": "https://colah.github.io/",
          "affiliation": "OpenAI",
          "affiliationURL": "https://openai.com"
        }
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <d-title>
    <h1>Understanding RL Vision</h1>
    <p>With diverse environments, we can analyze, diagnose and edit deep reinforcement learning models using attribution.</p>
    <figure id="hero" style="margin-top: 1em; margin-bottom: 0.5em; width: 100%;">
      <table style="margin-bottom: 1.5em; width: 100%;">
        <tr>
          <th>Observation <span style="font-weight: normal;">(video game still)</span></th>
          <th>Positive attribution <span style="font-weight: normal;">(good news)</span></th>
          <th>Negative attribution <span style="font-weight: normal;">(bad news)</span></th>
        </tr>
        <tr>
          <td style="width: 40%;" rowspan="2">
            <div class="hero-outer">
              <div class="hero-inner" style="z-index: 0;">
                <div id="hero-observation" class="hero-image" style="background-image: url('images/hero/observation.png');"></div>
              </div>
            </div>
          </td>
          <td style="height: 1px; width: 30%;">
            <div class="hero-outer">
              <div class="hero-inner" style="z-index: 0;">
                <div class="hero-image grayscale-light" style="background-image: url('images/hero/observation.png');"></div>
              </div>
              <div class="hero-inner" style="z-index: 1;">
                <div id="hero-overlay-pos" class="hero-image" style="background-image: url('images/hero/attribution_pos.png');"></div>
              </div>
            </div>
          </td>
          <td style="height: 1px; width: 30%;">
            <div class="hero-outer">
              <div class="hero-inner" style="z-index: 0;">
                <div class="hero-image grayscale-light" style="background-image: url('images/hero/observation.png');"></div>
              </div>
              <div class="hero-inner" style="z-index: 1;">
                <div id="hero-overlay-neg" class="hero-image" style="background-image: url('images/hero/attribution_neg.png');"></div>
              </div>
            </div>
          </td>
        </tr>
        <tr>
          <td colspan="2">
            <div id="hero-annotations" style="height: 100%; min-height: 100px;">
              <div style="width: 47%; text-align: left; line-height: 1.5em;">
                Attribution from a hidden layer to the value function, showing what features of the observation (left) are used to predict success (middle) and failure (right). Applying dimensionality reduction (NMF) yields features that detect various in-game objects.
              </div>
              <div style="top: 0px; left: 49%;">
                <img id="hero-annotation-1" src="images/hero/coin.png" class="hero-annotation-image">
                <br>
                <div class="hero-annotation-dot" style="background-color: #ffe400;"></div> Coin
                <div id="hero-annotation-lines-1" style="display: block;">
                  <div class="hero-annotation-line-vertical-outer">
                    <div class="hero-annotation-line-vertical-inner"></div>
                  </div>
                  <div class="hero-annotation-line-horizontal-outer" style="left: -23px; width: 76px;">
                    <div class="hero-annotation-line-horizontal-inner" style="width: 74px;"></div>
                  </div>
                </div>
              </div>
              <div style="top: 0px; left: 67.75%;">
                <img id="hero-annotation-2" src="images/hero/enemy.png" class="hero-annotation-image">
                <br>
                <div class="hero-annotation-dot" style="background-color: #a1ff00;"></div> Enemy
                <div id="hero-annotation-lines-2" style="display: block;">
                  <div class="hero-annotation-line-vertical-outer">
                    <div class="hero-annotation-line-vertical-inner"></div>
                  </div>
                  <div class="hero-annotation-line-horizontal-outer" style="left: 47px; width: 27px;">
                    <div class="hero-annotation-line-horizontal-inner" style="left: 2px; width: 25px;"></div>
                  </div>
                </div>
              </div>
              <div style="top: 0px; left: 86.5%;">
                <img id="hero-annotation-0" src="images/hero/saw.png" class="hero-annotation-image">
                <br>
                <div class="hero-annotation-dot" style="background-color: #ff0000;"></div> Buzzsaw
                <div id="hero-annotation-lines-0" style="display: block;">
                  <div class="hero-annotation-line-vertical-outer">
                    <div class="hero-annotation-line-vertical-inner"></div>
                  </div>
                  <div class="hero-annotation-line-horizontal-outer" style="left: 5px; width: 48px;">
                    <div class="hero-annotation-line-horizontal-inner" style="width: 46px;"></div>
                  </div>
                </div>
              </div>
            </div>
          </td>
        </tr>
      </table>
    </figure>
  </d-title>

  <d-article>

    <d-contents>
      <nav class="l-text figcaption">
        <h3>Contents</h3>
        <div><a href="#introduction">Introduction</a></div>
        <div><a href="#coinrun">Our CoinRun model</a></div>
        <div><a href="#analysis">Model analysis</a></div>
        <ul>
          <li><a href="#dissecting-failure">Dissecting failure</a></li>
          <li><a href="#hallucinations">Hallucinations</a></li>
          <li><a href="#model-editing">Model editing</a></li>
        </ul>
        <div><a href="#diversity-hypothesis">The diversity hypothesis</a></div>
        <div><a href="#feature-visualization">Feature visualization</a></div>
        <div><a href="#attribution">Attribution</a></div>
        <div><a href="#questions">Questions for further research</a></div>
      </nav>
    </d-contents>

    <div>
      <p id="introduction">
        In this article, we apply interpretability techniques to a reinforcement learning (RL) model trained to play the video game CoinRun <d-cite key="coinrunpaper"></d-cite>. Using attribution <d-cite key="attribution1,attribution2,attribution3,gradcam,attribution4,attribution5,attribution6,integratedgradients"></d-cite> combined with dimensionality reduction as in <d-cite key="buildingblocks"></d-cite>, we build an interface for exploring the objects detected by the model, and how they influence its value function and policy. We leverage this interface in several ways.
      </p>
      <ul>
        <li><b><a href="#dissecting-failure">Dissecting failure</a>.</b> We perform a step-by-step analysis of the agent's behavior in cases where it failed to achieve the maximum reward, allowing us to understand what went wrong, and why. For example, one case of failure was caused by an obstacle being temporarily obscured from view.</li>
        <li><b><a href="#hallucinations">Hallucinations</a>.</b> We find situations when the model "hallucinated" a feature not present in the observation, thereby explaining inaccuracies in the model's value function. These were brief enough that they did not affect the agent's behavior.</li>
        <li><b><a href="#model-editing">Model editing</a>.</b> We hand-edit the weights of the model to blind the agent to certain hazards, without otherwise changing the agent's behavior. We verify the effects of these edits by checking which hazards cause the new agents to fail. Such editing is only made possible by our previous analysis, and thus provides a quantitative validation of this analysis.</li>
      </ul>
      <p>
        Our results depend on levels in CoinRun being procedurally-generated, leading us to formulate a <span style="font-style: italic;"><a href="#diversity-hypothesis">diversity hypothesis</a></span> for interpretability. If it is correct, then we can expect RL models to become more interpretable as the environments they are trained on become more diverse. We provide evidence for our hypothesis by measuring the relationship between interpretability and generalization.
      </p>
      <p>
        Finally, we provide a thorough <a href="#feature-visualization">investigation</a> of several interpretability techniques in the context of RL vision, and pose a number of <a href="#questions">questions</a> for further research.
      </p>
    </div>
    <h2 id="coinrun">Our CoinRun model</h2>
    <p>
      CoinRun is a side-scrolling platformer in which the agent must dodge enemies and other traps and collect the coin at the end of the level.
    </p>
    <figure>
      <video style="width: 100%;" loop controls muted>
        <source src="videos/coinrun.mp4" type="video/mp4">
      </video>
      <figcaption style="text-align: center;">Our trained model playing CoinRun. <b>Left</b>: full resolution. <b>Right</b>: 64x64 RGB observations given to the model.</figcaption>
    </figure>
    <p>
      CoinRun is procedurally-generated, meaning that each new level encountered by the agent is randomly generated from scratch. This incentivizes the model to learn how to spot the different kinds of objects in the game, since it cannot get away with simply memorizing a small number of specific trajectories <d-cite key="procgen"></d-cite>.<d-footnote>We use the original version of CoinRun <d-cite key="coinrunpaper"></d-cite>, not the version from Procgen Benchmark <d-cite key="procgen"></d-cite>, which is slightly different. To play CoinRun yourself, please follow the instructions <a href="https://github.com/openai/coinrun">here</a>.</d-footnote>
    </p>
    <p>
      Here are some examples of the objects used, along with walls and floors, to generate CoinRun levels.
    </p>
    <figure id="coinrun-objects">
      <table>
        <tr class="coinrun-objects-row-images">
          <td style="font-weight: bold; white-space: normal;">Full resolution</td>
          <td>
            <img src="images/coinrun/agent_0_human.png" class="coinrun-object-double">
            <img src="images/coinrun/agent_1_human.png" class="coinrun-object-double">
          </td>
          <td>
            <img src="images/coinrun/coin_1_human.png" class="coinrun-object-single">
          </td>
          <td>
            <img src="images/coinrun/saw_1_human.png" class="coinrun-object-single">
          </td>
          <td>
            <img src="images/coinrun/enemy_0_human.png" class="coinrun-object-double">
            <img src="images/coinrun/enemy_1_human.png" class="coinrun-object-double">
          </td>
          <td>
            <img src="images/coinrun/box_0_human.png" class="coinrun-object-double">
            <img src="images/coinrun/box_1_human.png" class="coinrun-object-double">
          </td>
          <td>
            <img src="images/coinrun/lava_human.png" class="coinrun-object-single">
          </td>
          <td>
            <img src="images/coinrun/velocity_info_human.png" class="coinrun-object-single">
          </td>
        </tr>
        <tr class="coinrun-objects-row-images">
          <td style="font-weight: bold; white-space: normal;">Model resolution</td>
          <td>
            <img src="images/coinrun/agent_0_model.png" class="coinrun-object-double">
            <img src="images/coinrun/agent_1_model.png" class="coinrun-object-double">
          </td>
          <td>
            <img src="images/coinrun/coin_1_model.png" class="coinrun-object-single">
          </td>
          <td>
            <img src="images/coinrun/saw_1_model.png" class="coinrun-object-single">
          </td>
          <td>
            <img src="images/coinrun/enemy_0_model.png" class="coinrun-object-double">
            <img src="images/coinrun/enemy_1_model.png" class="coinrun-object-double">
          </td>
          <td>
            <img src="images/coinrun/box_0_model.png" class="coinrun-object-double">
            <img src="images/coinrun/box_1_model.png" class="coinrun-object-double">
          </td>
          <td>
            <img src="images/coinrun/lava_model.png" class="coinrun-object-single">
          </td>
          <td>
            <img src="images/coinrun/velocity_info_model.png" class="coinrun-object-single">
          </td>
        </tr>
        <tr class="coinrun-objects-row-text">
          <td></td>
          <td><figcaption>The agent, in mid air (left) and about to jump (right). The agent also appears in beige, blue and green.</figcaption></td>
          <td><figcaption>Coins, which have to be collected.</figcaption></td>
          <td><figcaption>Stationary buzzsaw obstacles, which must be dodged.</figcaption></td>
          <td><figcaption>Enemies, which must be dodged, moving left and right. There are several alternative sprites, all with white trails.</figcaption></td>
          <td><figcaption>Boxes, which the agent can both move past and land on top of.</figcaption></td>
          <td><figcaption>Lava at the bottom of a chasm.</figcaption></td>
          <td><figcaption>The velocity info painted into the top left of each observation, indicating the agent's horizontal and vertical velocities.<d-footnote class="velocity-info-footnote">Painting in the velocity info allows the model to infer the agent's motion from a single frame. The shade of the left square indicates the agent's horizontal velocity (black for left at full speed, white for right at full speed), and the shade of the right square indicates the agent's vertical velocity (black for down at full speed, white for up at full speed). In this example, the agent is moving forward and about to land (and is thus moving right and down).</d-footnote></figcaption></td>
        </tr>
      </table>
    </figure>
    <p>
      There are 9 actions available to the agent in CoinRun:
    </p>
    <figure id="coinrun-actions">
      <table>
        <tr>
          <td><span class="coinrun-action">&#8592;</span></td>
          <td><span class="coinrun-action">&#8594;</span></td>
          <td></td>
          <td><figcaption>Left and right change the agent's horizontal velocity. They still work while the agent is in mid-air, but have less of an effect.</figcaption></td>
        </tr>
        <tr>
          <td><span class="coinrun-action">&#8595;</span></td>
          <td></td>
          <td></td>
          <td><figcaption>Down cancels a jump if used immediately after up, and steps the agent down from boxes.</figcaption></td>
        </tr>
        <tr>
          <td><span class="coinrun-action">&#8593;</span></td>
          <td><span class="coinrun-action">&#8598;</span></td>
          <td><span class="coinrun-action">&#8599;</span></td>
          <td><figcaption>Up causes the agent to jump after the next non-up action. Diagonal directions have the same effect as both component directions combined.</figcaption></td>
        </tr>
        <tr>
          <td><span class="coinrun-action">A</span></td>
          <td><span class="coinrun-action">B</span></td>
          <td><span class="coinrun-action">C</span></td>
          <td><figcaption>A, B and C do nothing.<d-footnote>The original version of CoinRun only has 1 "do nothing" action, but our version ended up with 3 when "A" and "B" actions were added to be used in other games. For consistency, we have relabeled the original "do nothing" action as "C".</d-footnote></figcaption></td>
        </tr>
      </table>
    </figure>
    <p>
      We trained a convolutional neural network on CoinRun for around 2 billion timesteps, using PPO <d-cite key="ppo"></d-cite>, an actor-critic algorithm.<d-footnote>We used the standard PPO hyperparameters for CoinRun <d-cite key="coinrunpaper"></d-cite>, except that we used twice as many copies of the environment per worker and twice and many workers. The effect of these changes was to increase the effective batch size, which seemed to be necessary to reach the same performance with our smaller architecture.</d-footnote> The architecture of our network is described in <a href="#architecture">Appendix C</a>. We used a non-recurrent network, to avoid any need to visualize multiple frames at once. Thus our model observes a single downsampled 64x64 image, and outputs a value function (an estimate of the total future time-discounted reward) and a policy (a probability distribution over the actions, from which the next action is sampled).
    </p>
    <figure style="text-align: center;">
      <svg height="160px" width="448px">
        <defs>
          <marker id="arrow-model" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto" markerUnits="strokeWidth">
            <path d="M0,0 L0,6 L9,3 z" fill="#000000" />
          </marker>
        </defs>
        <image xlink:href="images/coinrun/observation.png" x="8" y="16" height="96" width="96" style="image-rendering: pixelated;"></image>
        <rect x="8" y="16" height="96" width="96" fill="none" stroke="#808080" stroke-width="1"></rect>
        <text x="56" y="135" text-anchor="middle">observation</text>
        <line x1="104" y1="64" x2="152" y2="64" stroke="#000000" stroke-width="1" marker-end="url(#arrow-model)"></line>
        <text x="180" y="69" text-anchor="middle">CNN</text>
        <rect x="152" y="36" height="56" width="56" fill="none" stroke="#000000" stroke-width="1"></rect>
        <path d="M 208 64 h 24 q 8 0, 8 -8 v -40 q 0 -8, 8 -8 h 24" fill="none" stroke="#000000" stroke-width="1" marker-end="url(#arrow-model)"></path>
        <path d="M 232 64 q 8 0, 8 8 q 0 8, 8 8 h 24" fill="none" stroke="#000000" stroke-width="1" marker-end="url(#arrow-model)"></path>
        <rect x="278" y="0" height="16" width="16" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="4" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <text x="300" y="13" text-anchor="start">value function</text>
        <rect x="278" y="24" height="112" width="16" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="28" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="40" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="52" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="64" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="76" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="88" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="100" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="112" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <rect x="282" y="124" height="8" width="8" fill="none" stroke="#000000" stroke-width="1"></rect>
        <text x="286" y="152" text-anchor="middle">logits</text>
        <line x1="300" y1="80" x2="380" y2="80" stroke="#000000" stroke-width="1" marker-end="url(#arrow-model)"></line>
        <text x="340" y="102" text-anchor="middle">softmax</text>
        <text x="388" y="84" text-anchor="start">policy</text>
      </svg>
      <figcaption>Schematic of a typical non-recurrent convolutional actor-critic model, such as ours.</figcaption>
    </figure>
    <p>
      Since the only available reward is a fixed bonus for collecting the coin, the value function estimates the time-discounted<d-footnote>We use a discount rate of 0.999 per timestep.</d-footnote> probability that the agent will successfully complete the level.
    </p>
    <h2 id="analysis">Model analysis</h2>
    <p>
      Having trained a strong RL agent, we were curious to see what it had learned. Following <d-cite key="buildingblocks"></d-cite>, we developed an interface for examining trajectories of the agent playing the game. This incorporates attribution from a hidden layer that recognizes objects, which serves to highlight objects that positively or negatively influence a particular network output. By applying dimensionality reduction, we obtain attribution vectors whose components correspond to different types of object, which we indicate using different colors.
    </p>
    <p id="interface">
      Here is our interface for a typical trajectory, with the value function as the network output. It reveals the model using obstacles, coins, enemies and more to compute the value function.
    </p>
    <d-figure style="grid-column: page; margin-bottom: 3em;">
      <figure>
        <div id="interface-header-target"></div>
      </figure>
    </d-figure>
    <h3 id="dissecting-failure">Dissecting failure</h3>
    <p>
      Our fully-trained model fails to complete around 1 in every 200 levels. We explored a few of these failures using our interface, and found that we were usually able to understand why they occurred.
    </p>
    <p>
      The failure often boils down to the fact that the model has no memory, and must therefore choose its action based only on the current observation. It is also common for some unlucky sampling of actions from the agent's policy to be partly responsible.
    </p>
    <p>
      Here are some cherry-picked examples of failures, carefully analyzed step-by-step.
    </p>
    <table style="margin-top: 1em; margin-bottom: 1em;">
      <tr>
        <td style="width: 50%; vertical-align: top;">
          <label id="interface-failure-obscured-label" style="color: black;">
            <b><input type="radio" name="interface-failure-options" checked id="interface-failure-obscured-option"> Buzzsaw obstacle obscured by enemy</b>
          </label><br>
          <label id="interface-failure-down-label" style="color: lightgray;">
            <b><input type="radio" name="interface-failure-options" id="interface-failure-down-option"> Stepping down to avoid jumping</b>
          </label><br>
          <label id="interface-failure-offscreen-label" style="color: lightgray;">
            <b><input type="radio" name="interface-failure-options" id="interface-failure-offscreen-option"> Landing platform moving off-screen</b>
          </label>
        </td>
        <td style="vertical-align: top;">
          <div style="min-height: 7.5em;">
            <div style="display: block;" id="interface-failure-obscured-text">
              The agent moves too far to the right while in mid-air as a result of a buzzsaw obstacle being temporarily hidden from view by a moving enemy. The buzzsaw comes back into view, but too late to avoid a collision.
            </div>
            <div style="display: none;" id="interface-failure-down-text">
              The agent presses down in a bid to delay a jump. This causes the agent to inadvertently step down from a box and onto an enemy.
            </div>
            <div style="display: none;" id="interface-failure-offscreen-text">
              The agent fails to move far enough to the right while in mid-air, as a result of the platform where it was intending to land moving below the field of view.
            </div>
          </div>
        </td>
      </tr>
    </table>
    <div style="display: flex; flex-flow: row; height: 116px;">
      <div>
        <div style="display: flex; flex-flow: row;">
          <div style="display: inline-block; margin-right:3px;">
            <button id="interface-failure-previous" class="interface-failure-step-button">Prev</button><br>
            <button id="interface-failure-start" class="interface-failure-step-button">Start</button>
          </div>
          <button id="interface-failure-play-pause-button" class="play-pause-button">
            <span id="interface-failure-play-pause-span">&#9658;</span>
          </button>
          <div style="display: inline-block; margin-left:3px;">
            <button id="interface-failure-next" class="interface-failure-step-button">Next</button><br>
            <button id="interface-failure-end" class="interface-failure-step-button">End</button>
          </div>
          <br />
        </div>
        <span id="interface-failure-position"></span>
      </div>
      <div id="interface-failure-description" style="position: relative; height: 115px;"></div>
    </div>
    <d-figure style="grid-column: page; margin-top: 1em;">
      <figure>
        <div id="interface-failure-obscured-target" style="display: block;"></div>
        <div id="interface-failure-down-target" style="display: none;"></div>
        <div id="interface-failure-offscreen-target" style="display: none;"></div>
      </figure>
    </d-figure>
    <h3 id="hallucinations">Hallucinations</h3>
    <p>
      We searched for errors in the model using generalized advantage estimation (GAE) <d-cite key="gae"></d-cite>,<d-footnote>We use the same GAE hyperparameters as in training, namely <d-math>\gamma=0.999</d-math> and <d-math>\lambda=0.95</d-math>.</d-footnote> which measures how successful each action turned out relative to the agent's expectations. An unusually high or low GAE indicates that either something unexpected occurred, or the agent's expectations were miscalibrated. Filtering for such timesteps can therefore find problems with the value function or policy.
    </p>
    <p>
      Using our interface, we found a couple of cases in which the model "hallucinated" a feature not present in the observation, causing the value function to spike.
    </p>
    <table style="margin-top: 1em; margin-bottom: 0em;">
      <tr>
        <td style="width: 30%; vertical-align: top;">
          <label id="interface-bug-coin-label" style="color: black;">
            <b><input type="radio" name="interface-bug-options" id="interface-bug-coin-option" checked> Coin hallucination</b>
          </label><br>
          <label id="interface-bug-saw-label" style="color: lightgray;">
            <b><input type="radio" name="interface-bug-options" id="interface-bug-saw-option"> Buzzsaw hallucination</b>
          </label>
        </td>
        <td style="vertical-align: top;">
          <div>
            <div style="display: block;" id="interface-bug-coin-text">
              At one point the value function spiked upwards from 95% to 98% for a single timestep. This was due to a curved yellow-brown shape in the background, which happened to appear next to a wall, being mistaken for a coin.
            </div>
            <div style="display: none;" id="interface-bug-saw-text">
              At another point the value function spiked downwards from 94% to 85% for a single timestep. This was due to the agent, colored in gray-blue and crouching against a mottled background, being mistaken for a buzzsaw obstacle. An actual buzzsaw was also present in the observation, but the main effect was from the misjudged agent, as shown by the larger red circle around the agent (hover over the first legend item to isolate).
            </div>
          </div>
        </td>
      </tr>
    </table>
    <d-figure style="grid-column: page; margin-top: 1em;">
      <figure>
        <div id="interface-bug-coin-target" style="display: block;"></div>
        <div id="interface-bug-saw-target" style="display: none;"></div>
      </figure>
    </d-figure>
    <h3 id="model-editing">Model editing</h3>
    <p>
      Our analysis so far has been mostly qualitative. To quantitatively validate our analysis, we hand-edited the model to make the agent blind to certain features identified by our interface: buzzsaw obstacles in one case, and left-moving enemies in another. Our method for this can be thought of as a primitive form of <a href="https://distill.pub/2020/circuits/">circuit</a>-editing <d-cite key="circuits"></d-cite>, and we explain it in detail in <a href="#model-editing-method">Appendix A</a>.
    </p>
    <p>
      We evaluated each edit by measuring the percentage of levels that the new agent failed to complete, broken down by the object that the agent collided with to cause the failure. Our results show that our edits were successful and targeted, with no statistically measurable effects on the agent's other abilities.<d-footnote>The data for this plot are as follows.<br>Percentage of levels failed due to: buzzsaw obstacle / enemy moving left / enemy moving right / multiple or other:<br>- Original model: 0.37% / 0.16% / 0.12% / 0.08%<br>- Buzzsaw obstacle blindness: 12.76% / 0.16% / 0.08% / 0.05%<br>- Enemy moving left blindness: 0.36% / 4.69% / 0.97% / 0.07%<br>Each model was tested on 10,000 levels.</d-footnote>
    </p>
    <figure class="matplotlib-svg" style="text-align: center;">
      <svg version="1.1" viewBox="0 0 426.942187 280.45375" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><style type="text/css"> *{stroke-linecap:butt;stroke-linejoin:round;} </style></defs><g id="figure_1"><g id="patch_1"><path d="M 0 280.45375 L 426.942187 280.45375 L 426.942187 0 L 0 0 z " style="fill:#ffffff;"></path></g><g id="axes_1"><g id="patch_2"><path d="M 50.104688 245.377813 L 416.242188 245.377813 L 416.242188 23.837812 L 50.104688 23.837812 z " style="fill:#ffffff;"></path></g><g id="patch_3"><path clip-path="url(#p780c04eea3)" d="M 66.747301 245.377813 L 90.522463 245.377813 L 90.522463 239.25975 L 66.747301 239.25975 z " style="fill:#df6060;"></path></g><g id="patch_4"><path clip-path="url(#p780c04eea3)" d="M 185.623113 245.377813 L 209.398275 245.377813 L 209.398275 34.387336 L 185.623113 34.387336 z " style="fill:#df6060;"></path></g><g id="patch_5"><path clip-path="url(#p780c04eea3)" d="M 304.498925 245.377813 L 328.274087 245.377813 L 328.274087 239.425103 L 304.498925 239.425103 z " style="fill:#df6060;"></path></g><g id="patch_6"><path clip-path="url(#p780c04eea3)" d="M 90.522463 245.377813 L 114.297626 245.377813 L 114.297626 242.732164 L 90.522463 242.732164 z " style="fill:#b0df60;"></path></g><g id="patch_7"><path clip-path="url(#p780c04eea3)" d="M 209.398275 245.377813 L 233.173437 245.377813 L 233.173437 242.732164 L 209.398275 242.732164 z " style="fill:#b0df60;"></path></g><g id="patch_8"><path clip-path="url(#p780c04eea3)" d="M 328.274087 245.377813 L 352.049249 245.377813 L 352.049249 167.827238 L 328.274087 167.827238 z " style="fill:#b0df60;"></path></g><g id="patch_9"><path clip-path="url(#p780c04eea3)" d="M 114.297626 245.377813 L 138.072788 245.377813 L 138.072788 243.393576 L 114.297626 243.393576 z " style="fill:#60df87;"></path></g><g id="patch_10"><path clip-path="url(#p780c04eea3)" d="M 233.173437 245.377813 L 256.9486 245.377813 L 256.9486 244.054988 L 233.173437 244.054988 z " style="fill:#60df87;"></path></g><g id="patch_11"><path clip-path="url(#p780c04eea3)" d="M 352.049249 245.377813 L 375.824412 245.377813 L 375.824412 229.338568 L 352.049249 229.338568 z " style="fill:#60df87;"></path></g><g id="patch_12"><path clip-path="url(#p780c04eea3)" d="M 138.072788 245.377813 L 161.84795 245.377813 L 161.84795 244.054988 L 138.072788 244.054988 z " style="fill:#1f77b4;"></path></g><g id="patch_13"><path clip-path="url(#p780c04eea3)" d="M 256.9486 245.377813 L 280.723762 245.377813 L 280.723762 244.551047 L 256.9486 244.551047 z " style="fill:#1f77b4;"></path></g><g id="patch_14"><path clip-path="url(#p780c04eea3)" d="M 375.824412 245.377813 L 399.599574 245.377813 L 399.599574 244.220341 L 375.824412 244.220341 z " style="fill:#1f77b4;"></path></g><g id="matplotlib.axis_1"><g id="xtick_1"><g id="line2d_1"><defs><path d="M 0 0 L 0 3.5 " id="m3850f438ae" style="stroke:#000000;stroke-width:0.8;"></path></defs><g><use style="stroke:#000000;stroke-width:0.8;" x="114.297626" xlink:href="#m3850f438ae" y="245.377813"></use></g></g><g id="text_1"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-0, 114.297626, 259.97625)" x="114.297626" y="259.97625">Original model</text></g></g><g id="xtick_2"><g id="line2d_2"><g><use style="stroke:#000000;stroke-width:0.8;" x="233.173437" xlink:href="#m3850f438ae" y="245.377813"></use></g></g><g id="text_2"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;" transform="translate(188.901562 259.97625)">Buzzsaw obstacle</text><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;" transform="translate(209.423437 271.174063)">blindness</text></g></g><g id="xtick_3"><g id="line2d_3"><g><use style="stroke:#000000;stroke-width:0.8;" x="352.049249" xlink:href="#m3850f438ae" y="245.377813"></use></g></g><g id="text_3"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;" transform="translate(304.830499 259.97625)">Enemy moving left</text><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;" transform="translate(328.299249 271.174063)">blindness</text></g></g></g><g id="matplotlib.axis_2"><g id="ytick_1"><g id="line2d_4"><defs><path d="M 0 0 L -3.5 0 " id="m11790fd33d" style="stroke:#000000;stroke-width:0.8;"></path></defs><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="245.377813"></use></g></g><g id="text_4"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 249.177031)" x="43.104688" y="249.177031">0%</text></g></g><g id="ytick_2"><g id="line2d_5"><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="212.307205"></use></g></g><g id="text_5"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 216.106424)" x="43.104688" y="216.106424">2%</text></g></g><g id="ytick_3"><g id="line2d_6"><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="179.236597"></use></g></g><g id="text_6"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 183.035816)" x="43.104688" y="183.035816">4%</text></g></g><g id="ytick_4"><g id="line2d_7"><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="146.16599"></use></g></g><g id="text_7"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 149.965209)" x="43.104688" y="149.965209">6%</text></g></g><g id="ytick_5"><g id="line2d_8"><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="113.095382"></use></g></g><g id="text_8"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 116.894601)" x="43.104688" y="116.894601">8%</text></g></g><g id="ytick_6"><g id="line2d_9"><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="80.024775"></use></g></g><g id="text_9"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 83.823993)" x="43.104688" y="83.823993">10%</text></g></g><g id="ytick_7"><g id="line2d_10"><g><use style="stroke:#000000;stroke-width:0.8;" x="50.104688" xlink:href="#m11790fd33d" y="46.954167"></use></g></g><g id="text_10"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 43.104688, 50.753386)" x="43.104688" y="50.753386">12%</text></g></g><g id="text_11"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-90, 14.798438, 134.607813)" x="14.798438" y="134.607813">Percentage of levels failed</text></g></g><g id="patch_15"><path d="M 50.104688 245.377813 L 50.104688 23.837813 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_16"><path d="M 416.242188 245.377813 L 416.242188 23.837813 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_17"><path d="M 50.104687 245.377813 L 416.242188 245.377813 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_18"><path d="M 50.104687 23.837812 L 416.242188 23.837812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="text_12"><text style="font-family:DejaVu Sans;font-size:14px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-0, 233.173438, 17.837812)" x="233.173438" y="17.837812">Failure rate by cause</text></g><g id="legend_1"><g id="patch_19"><path d="M 275.679688 105.228437 L 409.242188 105.228437 Q 411.242188 105.228437 411.242188 103.228437 L 411.242188 30.837812 Q 411.242188 28.837812 409.242188 28.837812 L 275.679688 28.837812 Q 273.679688 28.837812 273.679688 30.837812 L 273.679688 103.228437 Q 273.679688 105.228437 275.679688 105.228437 z " style="fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;"></path></g><g id="text_13"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:bold;text-anchor:start;" transform="rotate(-0, 322.514844, 40.43625)" x="322.514844" y="40.43625">Causes</text></g><g id="patch_20"><path d="M 277.679688 55.114375 L 297.679688 55.114375 L 297.679688 48.114375 L 277.679688 48.114375 z " style="fill:#df6060;"></path></g><g id="text_14"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 305.679688, 55.114375)" x="305.679688" y="55.114375">Buzzsaw obstacle</text></g><g id="patch_21"><path d="M 277.679688 69.7925 L 297.679688 69.7925 L 297.679688 62.7925 L 277.679688 62.7925 z " style="fill:#b0df60;"></path></g><g id="text_15"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 305.679688, 69.7925)" x="305.679688" y="69.7925">Enemy moving left</text></g><g id="patch_22"><path d="M 277.679688 84.470625 L 297.679688 84.470625 L 297.679688 77.470625 L 277.679688 77.470625 z " style="fill:#60df87;"></path></g><g id="text_16"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 305.679688, 84.470625)" x="305.679688" y="84.470625">Enemy moving right</text></g><g id="patch_23"><path d="M 277.679688 99.14875 L 297.679688 99.14875 L 297.679688 92.14875 L 277.679688 92.14875 z " style="fill:#1f77b4;"></path></g><g id="text_17"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 305.679688, 99.14875)" x="305.679688" y="99.14875">Multiple or other</text></g></g></g></g><defs><clipPath id="p780c04eea3"><rect height="221.54" width="366.1375" x="50.104688" y="23.837812"></rect></clipPath></defs></svg>
      <figcaption>Results of testing each model on 10,000 levels. Note that moving enemies can change direction.</figcaption>
    </figure>
    <p>
      We did not manage to achieve complete blindness, however: the buzzsaw-edited model still performed significantly better than the original model did when we made the buzzsaws completely invisible.<d-footnote>Our results on the version of the game with invisible buzzsaws are as follows.<br>Percentage of levels failed due to: buzzsaw obstacle / enemy moving left / enemy moving right / multiple or other:<br>Original model, invisible buzzsaws: 32.20% / 0.05% / 0.05% / 0.05%<br>We tested the model on 10,000 levels.<br>We experimented briefly with iterating the editing procedure, but were not able to achieve more than around 50% buzzsaw blindness by this metric without affecting the model's other abilities.</d-footnote> This implies that the model has other ways of detecting buzzsaws than the feature identified by our interface.
    </p>
    <p>
      Here are the original and edited models playing some cherry-picked levels.
    </p>
    <figure id="model-editing-levels" style="grid-column: page;">
      <table>
        <tr>
          <td style="vertical-align: top;">
            <label id="model-editing-level-1-label" style="color: black;">
              <b><input type="radio" name="model-editing-level-options" checked id="model-editing-level-1-option"> Level 1</b>
            </label><br>
            <label id="model-editing-level-2-label" style="color: lightgray;">
              <b><input type="radio" name="model-editing-level-options" id="model-editing-level-2-option"> Level 2</b>
            </label><br>
            <label id="model-editing-level-3-label" style="color: lightgray;">
              <b><input type="radio" name="model-editing-level-options" id="model-editing-level-3-option"> Level 3</b>
            </label>
            <div style="margin-top: 1em;">
              <button id="model-editing-level-1-play-pause-button" class="play-pause-button" style="display: block; margin: 0 auto;">
                <span id="model-editing-level-1-play-pause-span">&#9658;</span>
              </button>
              <button id="model-editing-level-2-play-pause-button" class="play-pause-button" style="display: none; margin: 0 auto;">
                <span id="model-editing-level-2-play-pause-span">&#9658;</span>
              </button>
              <button id="model-editing-level-3-play-pause-button" class="play-pause-button" style="display: none; margin: 0 auto;">
                <span id="model-editing-level-3-play-pause-span">&#9658;</span>
              </button>
            </div>
          </td>
          <td>
            <div style="display: block;" id="model-editing-level-1-target-1">
              <video id="model-editing-level-1-video-1" controls muted>
                <source src="videos/model_editing/original_episode_0.mp4" type="video/mp4">
              </video>
            </div>
            <div style="display: none;" id="model-editing-level-2-target-1">
              <video id="model-editing-level-2-video-1" controls muted>
                <source src="videos/model_editing/original_episode_1.mp4" type="video/mp4">
              </video>
            </div>
            <div style="display: none;" id="model-editing-level-3-target-1">
              <video id="model-editing-level-3-video-1" controls muted>
                <source src="videos/model_editing/original_episode_2.mp4" type="video/mp4">
              </video>
            </div>
          </td>
          <td>
            <div style="display: block;" id="model-editing-level-1-target-2">
              <video id="model-editing-level-1-video-2" controls muted>
                <source src="videos/model_editing/saw_episode_0.mp4" type="video/mp4">
              </video>
            </div>
            <div style="display: none;" id="model-editing-level-2-target-2">
              <video id="model-editing-level-2-video-2" controls muted>
                <source src="videos/model_editing/saw_episode_1.mp4" type="video/mp4">
              </video>
            </div>
            <div style="display: none;" id="model-editing-level-3-target-2">
              <video id="model-editing-level-3-video-2" controls muted>
                <source src="videos/model_editing/saw_episode_2.mp4" type="video/mp4">
              </video>
            </div>
          </td>
          <td>
            <div style="display: block;" id="model-editing-level-1-target-3">
              <video id="model-editing-level-1-video-3" controls muted>
                <source src="videos/model_editing/enemy_episode_0.mp4" type="video/mp4">
              </video>
            </div>
            <div style="display: none;" id="model-editing-level-2-target-3">
              <video id="model-editing-level-2-video-3" controls muted>
                <source src="videos/model_editing/enemy_episode_1.mp4" type="video/mp4">
              </video>
            </div>
            <div style="display: none;" id="model-editing-level-3-target-3">
              <video id="model-editing-level-3-video-3" controls muted>
                <source src="videos/model_editing/enemy_episode_2.mp4" type="video/mp4">
              </video>
            </div>
          </td>
        </tr>
        <tr>
          <td></td>
          <td><figcaption>Original model.</figcaption></td>
          <td><figcaption>Buzzsaw obstacle blindness.</figcaption></td>
          <td><figcaption>Enemy moving left blindness.</figcaption></td>
        </tr>
      </table>
    </figure>
    <h2 id="diversity-hypothesis">The diversity hypothesis</h2>
    <p>
      All of the above analysis uses the same hidden layer of our network, the third of five convolutional layers, since it was much harder to find interpretable features at other layers. Interestingly, the level of abstraction at which this layer operates &ndash; finding the locations of various in-game objects &ndash; is exactly the level at which CoinRun levels are randomized using procedural generation. Furthermore, we found that training on many randomized levels was essential for us to be able to find any interpretable features at all.
    </p>
    <p>
      This led us to suspect that the diversity introduced by CoinRun's randomization is linked to the formation of interpretable features. We call this the <span style="font-style: italic;">diversity hypothesis</span>:
    </p>
    <blockquote>
      Interpretable features tend to arise (at a given level of abstraction) if and only if the training distribution is diverse enough (at that level of abstraction).
    </blockquote>
    <p>
      Our explanation for this hypothesis is as follows. For the forward implication ("only if"), we only expect features to be interpretable if they are general enough, and when the training distribution is not diverse enough, models have no incentive to develop features that generalize instead of overfitting. For the reverse implication ("if"), we do not expect it to hold in a strict sense: diversity on its own is not enough to guarantee the development of interpretable features, since they must also be relevant to the task. Rather, our intention with the reverse implication is to hypothesize that it holds very often in practice, as a result of generalization being bottlenecked by diversity.
    </p>
    <p>
      In CoinRun, procedural generation is used to incentivize the model to learn skills that generalize to unseen levels <d-cite key="procgen,gvgai,obstacletower"></d-cite>. However, only the <span style="font-style: italic;">layout</span> of each level is randomized, and correspondingly, we were only able to find interpretable features at the level of abstraction of objects. At a lower level, there are only a handful of visual patterns in the game, and the low-level features of our model seem to consist mostly of memorized color configurations used for picking these out. Similarly, the game's high-level dynamics follow a few simple rules, and accordingly the high-level features of our model seem to involve mixtures of combinations of objects that are hard to decipher. To explore the other convolutional layers, see the interface <a href="https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo/interface.html">here</a>.
    </p>
    <h3 id="interpretability-and-generalization">Interpretability and generalization</h3>
    <p>
      To test our hypothesis, we made the training distribution less diverse, by training the agent on a fixed set of 100 levels. This dramatically reduced our ability to interpret the model's features. Here we display an interface for the new model, generated in the same way as the one <a href="#interface">above</a>. The smoothly increasing value function suggests that the model has memorized the number of timesteps until the end of the level, and the features it uses for this focus on irrelevant background objects. Similar overfitting occurs for other video games with a limited number of levels <d-cite key="sonicsaliency"></d-cite>.
    </p>
    <d-figure style="grid-column: page; margin-bottom: 3em;">
      <figure>
        <div id="interface-100-levels-target"></div>
      </figure>
    </d-figure>
    <p>
      We attempted to quantify this effect by varying the number of levels used to train the agent, and evaluating the 8 features identified by our interface on how interpretable they were.<d-footnote>The interfaces used for this evaluation can be found <a href="https://openaipublic.blob.core.windows.net/rl-clarity/attribution/finite_levels/index.html">here</a>.</d-footnote> Features were scored based on how consistently they focused on the same objects, and whether the value function attribution made sense &ndash; for example, background objects should not be relevant. This process was subjective and noisy, but that may be unavoidable. We also measured the generalization ability of each model, by testing the agent on unseen levels <d-cite key="coinrunpaper"></d-cite>.<d-footnote>The data for this plot are as follows.<br>- Number of training levels: 100 / 300 / 1000 / 3,000 / 10,000 / 30,000 / 100,000<br>- Percentage of levels completed (train, run 1): 99.96% / 99.82% / 99.67% / 99.65% / 99.47% / 99.55% / 99.57%<br>- Percentage of levels completed (train, run 2): 99.97% / 99.86% / 99.70% / 99.46% / 99.39% / 99.50% / 99.37%<br>- Percentage of levels completed (test, run 1): 61.81% / 66.95% / 74.93% / 89.87% / 97.53% / 98.66% / 99.25%<br>- Percentage of levels completed (test, run 2): 64.13% / 67.64% / 73.46% / 90.36% / 97.44% / 98.89% / 99.35%<br>- Percentage of features interpretable (researcher 1, run 1): 52.5% / 22.5% / 11.25% / 45% / 90% / 75% / 91.25%<br>- Percentage of features interpretable (researcher 2, run 1): 8.75% / 8.75% / 10% / 26.25% / 56.25% / 90% / 70%<br>- Percentage of features interpretable (researcher 1, run 2): 15% / 13.75% / 15% / 23.75% / 53.75% / 90% / 96.25%<br>- Percentage of features interpretable (researcher 2, run 2): 3.75% / 6.25% / 21.25% / 45% / 72.5% / 83.75% / 77.5%<br>Percentages of levels completed are estimated by sampling 10,000 levels with replacement.</d-footnote>
    </p>
    <figure class="matplotlib-svg">
      <svg version="1.1" viewBox="0 0 447.734375 204.089062" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><style type="text/css"> *{stroke-linecap:butt;stroke-linejoin:round;} </style></defs><g id="figure_1"><g id="patch_1"><path d="M 0 204.089062 L 447.734375 204.089062 L 447.734375 0 L 0 0 z " style="fill:#ffffff;"></path></g><g id="axes_1"><g id="patch_2"><path d="M 56.467188 166.532812 L 208.649006 166.532812 L 208.649006 30.632812 L 56.467188 30.632812 z " style="fill:#ffffff;"></path></g><g id="PolyCollection_1"><defs><path d="M 63.384543 -167.278977 L 63.384543 -167.246602 L 85.387325 -166.793343 L 109.500245 -166.307708 L 131.503027 -165.627819 L 155.615948 -165.40119 L 177.61873 -165.757322 L 201.73165 -165.336439 L 201.73165 -165.983952 L 201.73165 -165.983952 L 177.61873 -165.9192 L 155.615948 -165.660195 L 131.503027 -166.242957 L 109.500245 -166.404835 L 85.387325 -166.922845 L 63.384543 -167.278977 z " id="mbc81a41cf8" style="stroke:#1f77b4;stroke-opacity:0.25;"></path></defs><g clip-path="url(#pd74eaed00d)"><use style="fill:#1f77b4;fill-opacity:0.25;stroke:#1f77b4;stroke-opacity:0.25;" x="0" xlink:href="#mbc81a41cf8" y="204.089062"></use></g></g><g id="PolyCollection_2"><defs><path d="M 63.384543 -51.244672 L 63.384543 -43.733523 L 85.387325 -60.374603 L 109.500245 -81.451147 L 131.503027 -134.579578 L 155.615948 -159.08794 L 177.61873 -163.037768 L 201.73165 -164.947931 L 201.73165 -165.271687 L 201.73165 -165.271687 L 177.61873 -163.782408 L 155.615948 -159.37932 L 131.503027 -136.165984 L 109.500245 -86.210367 L 85.387325 -62.608523 L 63.384543 -51.244672 z " id="mad45e18a1d" style="stroke:#ff7f0e;stroke-opacity:0.25;"></path></defs><g clip-path="url(#pd74eaed00d)"><use style="fill:#ff7f0e;fill-opacity:0.25;stroke:#ff7f0e;stroke-opacity:0.25;" x="0" xlink:href="#mad45e18a1d" y="204.089062"></use></g></g><g id="matplotlib.axis_1"><g id="xtick_1"><g id="line2d_1"><defs><path d="M 0 0 L 0 3.5 " id="me32769e7a0" style="stroke:#000000;stroke-width:0.8;"></path></defs><g><use style="stroke:#000000;stroke-width:0.8;" x="63.384543" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_1"><g transform="translate(54.584543 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.9765625">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-4.8046875">2</tspan></text></g></g></g><g id="xtick_2"><g id="line2d_2"><g><use style="stroke:#000000;stroke-width:0.8;" x="109.500245" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_2"><g transform="translate(100.700245 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.9765625">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-4.8046875">3</tspan></text></g></g></g><g id="xtick_3"><g id="line2d_3"><g><use style="stroke:#000000;stroke-width:0.8;" x="155.615948" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_3"><g transform="translate(146.815948 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.06406250000000036">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-3.8921875000000004">4</tspan></text></g></g></g><g id="xtick_4"><g id="line2d_4"><g><use style="stroke:#000000;stroke-width:0.8;" x="201.73165" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_4"><g transform="translate(192.93165 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.06406250000000036">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-3.8921875000000004">5</tspan></text></g></g></g><g id="xtick_5"><g id="line2d_5"><defs><path d="M 0 0 L 0 2 " id="mab6f2bdee0" style="stroke:#000000;stroke-width:0.6;"></path></defs><g><use style="stroke:#000000;stroke-width:0.6;" x="56.24113" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_6"><g id="line2d_6"><g><use style="stroke:#000000;stroke-width:0.6;" x="58.91547" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_7"><g id="line2d_7"><g><use style="stroke:#000000;stroke-width:0.6;" x="61.274404" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_8"><g id="line2d_8"><g><use style="stroke:#000000;stroke-width:0.6;" x="77.266753" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_9"><g id="line2d_9"><g><use style="stroke:#000000;stroke-width:0.6;" x="85.387325" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_10"><g id="line2d_10"><g><use style="stroke:#000000;stroke-width:0.6;" x="91.148962" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_11"><g id="line2d_11"><g><use style="stroke:#000000;stroke-width:0.6;" x="95.618036" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_12"><g id="line2d_12"><g><use style="stroke:#000000;stroke-width:0.6;" x="99.269534" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_13"><g id="line2d_13"><g><use style="stroke:#000000;stroke-width:0.6;" x="102.356833" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_14"><g id="line2d_14"><g><use style="stroke:#000000;stroke-width:0.6;" x="105.031172" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_15"><g id="line2d_15"><g><use style="stroke:#000000;stroke-width:0.6;" x="107.390107" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_16"><g id="line2d_16"><g><use style="stroke:#000000;stroke-width:0.6;" x="123.382455" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_17"><g id="line2d_17"><g><use style="stroke:#000000;stroke-width:0.6;" x="131.503027" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_18"><g id="line2d_18"><g><use style="stroke:#000000;stroke-width:0.6;" x="137.264665" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_19"><g id="line2d_19"><g><use style="stroke:#000000;stroke-width:0.6;" x="141.733738" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_20"><g id="line2d_20"><g><use style="stroke:#000000;stroke-width:0.6;" x="145.385237" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_21"><g id="line2d_21"><g><use style="stroke:#000000;stroke-width:0.6;" x="148.472535" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_22"><g id="line2d_22"><g><use style="stroke:#000000;stroke-width:0.6;" x="151.146875" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_23"><g id="line2d_23"><g><use style="stroke:#000000;stroke-width:0.6;" x="153.505809" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_24"><g id="line2d_24"><g><use style="stroke:#000000;stroke-width:0.6;" x="169.498158" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_25"><g id="line2d_25"><g><use style="stroke:#000000;stroke-width:0.6;" x="177.61873" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_26"><g id="line2d_26"><g><use style="stroke:#000000;stroke-width:0.6;" x="183.380367" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_27"><g id="line2d_27"><g><use style="stroke:#000000;stroke-width:0.6;" x="187.849441" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_28"><g id="line2d_28"><g><use style="stroke:#000000;stroke-width:0.6;" x="191.500939" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_29"><g id="line2d_29"><g><use style="stroke:#000000;stroke-width:0.6;" x="194.588238" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_30"><g id="line2d_30"><g><use style="stroke:#000000;stroke-width:0.6;" x="197.262577" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_31"><g id="line2d_31"><g><use style="stroke:#000000;stroke-width:0.6;" x="199.621511" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g></g><g id="matplotlib.axis_2"><g id="ytick_1"><g id="line2d_32"><defs><path d="M 0 0 L -3.5 0 " id="m7c0213d7c9" style="stroke:#000000;stroke-width:0.8;"></path></defs><g><use style="stroke:#000000;stroke-width:0.8;" x="56.467188" xlink:href="#m7c0213d7c9" y="166.215531"></use></g></g><g id="text_5"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 49.467188, 170.01475)" x="49.467188" y="170.01475">60%</text></g></g><g id="ytick_2"><g id="line2d_33"><g><use style="stroke:#000000;stroke-width:0.8;" x="56.467188" xlink:href="#m7c0213d7c9" y="133.839888"></use></g></g><g id="text_6"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 49.467188, 137.639107)" x="49.467188" y="137.639107">70%</text></g></g><g id="ytick_3"><g id="line2d_34"><g><use style="stroke:#000000;stroke-width:0.8;" x="56.467188" xlink:href="#m7c0213d7c9" y="101.464245"></use></g></g><g id="text_7"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 49.467188, 105.263463)" x="49.467188" y="105.263463">80%</text></g></g><g id="ytick_4"><g id="line2d_35"><g><use style="stroke:#000000;stroke-width:0.8;" x="56.467188" xlink:href="#m7c0213d7c9" y="69.088602"></use></g></g><g id="text_8"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 49.467188, 72.88782)" x="49.467188" y="72.88782">90%</text></g></g><g id="ytick_5"><g id="line2d_36"><g><use style="stroke:#000000;stroke-width:0.8;" x="56.467188" xlink:href="#m7c0213d7c9" y="36.712958"></use></g></g><g id="text_9"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:end;" transform="rotate(-0, 49.467188, 40.512177)" x="49.467188" y="40.512177">100%</text></g></g><g id="text_10"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-90, 14.798438, 98.582812)" x="14.798438" y="98.582812">Percentage of levels completed</text></g></g><g id="line2d_37"><path clip-path="url(#pd74eaed00d)" d="M 63.384543 36.826273 L 85.387325 37.230969 L 109.500245 37.732791 L 131.503027 38.153674 L 155.615948 38.55837 L 177.61873 38.250801 L 201.73165 38.428867 " style="fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;"></path><defs><path d="M 0 1.5 C 0.397805 1.5 0.77937 1.341951 1.06066 1.06066 C 1.341951 0.77937 1.5 0.397805 1.5 0 C 1.5 -0.397805 1.341951 -0.77937 1.06066 -1.06066 C 0.77937 -1.341951 0.397805 -1.5 0 -1.5 C -0.397805 -1.5 -0.77937 -1.341951 -1.06066 -1.06066 C -1.341951 -0.77937 -1.5 -0.397805 -1.5 0 C -1.5 0.397805 -1.341951 0.77937 -1.06066 1.06066 C -0.77937 1.341951 -0.397805 1.5 0 1.5 z " id="m2ae5715685" style="stroke:#1f77b4;"></path></defs><g clip-path="url(#pd74eaed00d)"><use style="fill:#1f77b4;stroke:#1f77b4;" x="63.384543" xlink:href="#m2ae5715685" y="36.826273"></use><use style="fill:#1f77b4;stroke:#1f77b4;" x="85.387325" xlink:href="#m2ae5715685" y="37.230969"></use><use style="fill:#1f77b4;stroke:#1f77b4;" x="109.500245" xlink:href="#m2ae5715685" y="37.732791"></use><use style="fill:#1f77b4;stroke:#1f77b4;" x="131.503027" xlink:href="#m2ae5715685" y="38.153674"></use><use style="fill:#1f77b4;stroke:#1f77b4;" x="155.615948" xlink:href="#m2ae5715685" y="38.55837"></use><use style="fill:#1f77b4;stroke:#1f77b4;" x="177.61873" xlink:href="#m2ae5715685" y="38.250801"></use><use style="fill:#1f77b4;stroke:#1f77b4;" x="201.73165" xlink:href="#m2ae5715685" y="38.428867"></use></g></g><g id="line2d_38"><path clip-path="url(#pd74eaed00d)" d="M 63.384543 156.599965 L 85.387325 142.597499 L 109.500245 120.258306 L 131.503027 68.716282 L 155.615948 44.855433 L 177.61873 40.678975 L 201.73165 38.979253 " style="fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;"></path><defs><path d="M 0 1.5 C 0.397805 1.5 0.77937 1.341951 1.06066 1.06066 C 1.341951 0.77937 1.5 0.397805 1.5 0 C 1.5 -0.397805 1.341951 -0.77937 1.06066 -1.06066 C 0.77937 -1.341951 0.397805 -1.5 0 -1.5 C -0.397805 -1.5 -0.77937 -1.341951 -1.06066 -1.06066 C -1.341951 -0.77937 -1.5 -0.397805 -1.5 0 C -1.5 0.397805 -1.341951 0.77937 -1.06066 1.06066 C -0.77937 1.341951 -0.397805 1.5 0 1.5 z " id="m3e55756d29" style="stroke:#ff7f0e;"></path></defs><g clip-path="url(#pd74eaed00d)"><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="63.384543" xlink:href="#m3e55756d29" y="156.599965"></use><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="85.387325" xlink:href="#m3e55756d29" y="142.597499"></use><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="109.500245" xlink:href="#m3e55756d29" y="120.258306"></use><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="131.503027" xlink:href="#m3e55756d29" y="68.716282"></use><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="155.615948" xlink:href="#m3e55756d29" y="44.855433"></use><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="177.61873" xlink:href="#m3e55756d29" y="40.678975"></use><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="201.73165" xlink:href="#m3e55756d29" y="38.979253"></use></g></g><g id="patch_3"><path d="M 56.467188 166.532812 L 56.467188 30.632812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_4"><path d="M 208.649006 166.532812 L 208.649006 30.632812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_5"><path d="M 56.467188 166.532812 L 208.649006 166.532812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_6"><path d="M 56.467188 30.632812 L 208.649006 30.632812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="text_11"><text style="font-family:DejaVu Sans;font-size:12px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-0, 132.558097, 24.632812)" x="132.558097" y="24.632812">Generalization</text></g><g id="legend_1"><g id="patch_7"><path d="M 144.208381 161.532812 L 201.649006 161.532812 Q 203.649006 161.532812 203.649006 159.532812 L 203.649006 131.176562 Q 203.649006 129.176562 201.649006 129.176562 L 144.208381 129.176562 Q 142.208381 129.176562 142.208381 131.176562 L 142.208381 159.532812 Q 142.208381 161.532812 144.208381 161.532812 z " style="fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;"></path></g><g id="line2d_39"><path d="M 146.208381 137.275 L 166.208381 137.275 " style="fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;"></path></g><g id="line2d_40"><g><use style="fill:#1f77b4;stroke:#1f77b4;" x="156.208381" xlink:href="#m2ae5715685" y="137.275"></use></g></g><g id="text_12"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 174.208381, 140.775)" x="174.208381" y="140.775">Train</text></g><g id="line2d_41"><path d="M 146.208381 151.953125 L 166.208381 151.953125 " style="fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;"></path></g><g id="line2d_42"><g><use style="fill:#ff7f0e;stroke:#ff7f0e;" x="156.208381" xlink:href="#m3e55756d29" y="151.953125"></use></g></g><g id="text_13"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 174.208381, 155.453125)" x="174.208381" y="155.453125">Test</text></g></g></g><g id="axes_2"><g id="patch_8"><path d="M 239.085369 166.532812 L 391.267187 166.532812 L 391.267187 30.632812 L 239.085369 30.632812 z " style="fill:#ffffff;"></path></g><g id="patch_9"><path clip-path="url(#p1682c8023d)" d="M 246.002725 166.532812 L 255.234838 166.532812 L 255.234838 140.523243 L 246.002725 140.523243 z " style="fill:#2ca02c;"></path></g><g id="patch_10"><path clip-path="url(#p1682c8023d)" d="M 266.537227 166.532812 L 275.769341 166.532812 L 275.769341 149.870432 L 266.537227 149.870432 z " style="fill:#2ca02c;"></path></g><g id="patch_11"><path clip-path="url(#p1682c8023d)" d="M 289.041056 166.532812 L 298.273169 166.532812 L 298.273169 147.838435 L 289.041056 147.838435 z " style="fill:#2ca02c;"></path></g><g id="patch_12"><path clip-path="url(#p1682c8023d)" d="M 309.575559 166.532812 L 318.807672 166.532812 L 318.807672 121.016066 L 309.575559 121.016066 z " style="fill:#2ca02c;"></path></g><g id="patch_13"><path clip-path="url(#p1682c8023d)" d="M 332.079387 166.532812 L 341.311501 166.532812 L 341.311501 77.937717 L 332.079387 77.937717 z " style="fill:#2ca02c;"></path></g><g id="patch_14"><path clip-path="url(#p1682c8023d)" d="M 352.61389 166.532812 L 361.846003 166.532812 L 361.846003 56.398542 L 352.61389 56.398542 z " style="fill:#2ca02c;"></path></g><g id="patch_15"><path clip-path="url(#p1682c8023d)" d="M 375.117719 166.532812 L 384.349832 166.532812 L 384.349832 57.617741 L 375.117719 57.617741 z " style="fill:#2ca02c;"></path></g><g id="matplotlib.axis_3"><g id="xtick_32"><g id="line2d_43"><g><use style="stroke:#000000;stroke-width:0.8;" x="250.579922" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_14"><g transform="translate(241.779922 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.9765625">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-4.8046875">2</tspan></text></g></g></g><g id="xtick_33"><g id="line2d_44"><g><use style="stroke:#000000;stroke-width:0.8;" x="293.618253" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_15"><g transform="translate(284.818253 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.9765625">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-4.8046875">3</tspan></text></g></g></g><g id="xtick_34"><g id="line2d_45"><g><use style="stroke:#000000;stroke-width:0.8;" x="336.656585" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_16"><g transform="translate(327.856585 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.06406250000000036">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-3.8921875000000004">4</tspan></text></g></g></g><g id="xtick_35"><g id="line2d_46"><g><use style="stroke:#000000;stroke-width:0.8;" x="379.694916" xlink:href="#me32769e7a0" y="166.532812"></use></g></g><g id="text_17"><g transform="translate(370.894916 181.13125)"><text><tspan style="font-family:DejaVu Sans;font-size:10px;font-style:book;font-weight:book;" x="0 6.362305" y="-0.06406250000000036">10</tspan><tspan style="font-family:DejaVu Sans;font-size:7px;font-style:book;font-weight:book;" x="12.820312" y="-3.8921875000000004">5</tspan></text></g></g></g><g id="xtick_36"><g id="line2d_47"><g><use style="stroke:#000000;stroke-width:0.6;" x="241.031922" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_37"><g id="line2d_48"><g><use style="stroke:#000000;stroke-width:0.6;" x="243.9132" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_38"><g id="line2d_49"><g><use style="stroke:#000000;stroke-width:0.6;" x="246.409077" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_39"><g id="line2d_50"><g><use style="stroke:#000000;stroke-width:0.6;" x="248.610596" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_40"><g id="line2d_51"><g><use style="stroke:#000000;stroke-width:0.6;" x="263.535751" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_41"><g id="line2d_52"><g><use style="stroke:#000000;stroke-width:0.6;" x="271.114425" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_42"><g id="line2d_53"><g><use style="stroke:#000000;stroke-width:0.6;" x="276.491579" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_43"><g id="line2d_54"><g><use style="stroke:#000000;stroke-width:0.6;" x="280.662425" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_44"><g id="line2d_55"><g><use style="stroke:#000000;stroke-width:0.6;" x="284.070253" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_45"><g id="line2d_56"><g><use style="stroke:#000000;stroke-width:0.6;" x="286.951532" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_46"><g id="line2d_57"><g><use style="stroke:#000000;stroke-width:0.6;" x="289.447408" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_47"><g id="line2d_58"><g><use style="stroke:#000000;stroke-width:0.6;" x="291.648927" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_48"><g id="line2d_59"><g><use style="stroke:#000000;stroke-width:0.6;" x="306.574082" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_49"><g id="line2d_60"><g><use style="stroke:#000000;stroke-width:0.6;" x="314.152756" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_50"><g id="line2d_61"><g><use style="stroke:#000000;stroke-width:0.6;" x="319.529911" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_51"><g id="line2d_62"><g><use style="stroke:#000000;stroke-width:0.6;" x="323.700756" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_52"><g id="line2d_63"><g><use style="stroke:#000000;stroke-width:0.6;" x="327.108585" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_53"><g id="line2d_64"><g><use style="stroke:#000000;stroke-width:0.6;" x="329.989863" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_54"><g id="line2d_65"><g><use style="stroke:#000000;stroke-width:0.6;" x="332.485739" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_55"><g id="line2d_66"><g><use style="stroke:#000000;stroke-width:0.6;" x="334.687259" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_56"><g id="line2d_67"><g><use style="stroke:#000000;stroke-width:0.6;" x="349.612413" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_57"><g id="line2d_68"><g><use style="stroke:#000000;stroke-width:0.6;" x="357.191087" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_58"><g id="line2d_69"><g><use style="stroke:#000000;stroke-width:0.6;" x="362.568242" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_59"><g id="line2d_70"><g><use style="stroke:#000000;stroke-width:0.6;" x="366.739087" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_60"><g id="line2d_71"><g><use style="stroke:#000000;stroke-width:0.6;" x="370.146916" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_61"><g id="line2d_72"><g><use style="stroke:#000000;stroke-width:0.6;" x="373.028194" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_62"><g id="line2d_73"><g><use style="stroke:#000000;stroke-width:0.6;" x="375.524071" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g><g id="xtick_63"><g id="line2d_74"><g><use style="stroke:#000000;stroke-width:0.6;" x="377.72559" xlink:href="#mab6f2bdee0" y="166.532812"></use></g></g></g></g><g id="matplotlib.axis_4"><g id="ytick_6"><g id="line2d_75"><defs><path d="M 0 0 L 3.5 0 " id="m1b3fe9340e" style="stroke:#000000;stroke-width:0.8;"></path></defs><g><use style="stroke:#000000;stroke-width:0.8;" x="391.267187" xlink:href="#m1b3fe9340e" y="166.532812"></use></g></g><g id="text_18"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 398.267187, 170.332031)" x="398.267187" y="170.332031">0%</text></g></g><g id="ytick_7"><g id="line2d_76"><g><use style="stroke:#000000;stroke-width:0.8;" x="391.267187" xlink:href="#m1b3fe9340e" y="140.523243"></use></g></g><g id="text_19"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 398.267187, 144.322462)" x="398.267187" y="144.322462">20%</text></g></g><g id="ytick_8"><g id="line2d_77"><g><use style="stroke:#000000;stroke-width:0.8;" x="391.267187" xlink:href="#m1b3fe9340e" y="114.513674"></use></g></g><g id="text_20"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 398.267187, 118.312892)" x="398.267187" y="118.312892">40%</text></g></g><g id="ytick_9"><g id="line2d_78"><g><use style="stroke:#000000;stroke-width:0.8;" x="391.267187" xlink:href="#m1b3fe9340e" y="88.504104"></use></g></g><g id="text_21"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 398.267187, 92.303323)" x="398.267187" y="92.303323">60%</text></g></g><g id="ytick_10"><g id="line2d_79"><g><use style="stroke:#000000;stroke-width:0.8;" x="391.267187" xlink:href="#m1b3fe9340e" y="62.494535"></use></g></g><g id="text_22"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 398.267187, 66.293754)" x="398.267187" y="66.293754">80%</text></g></g><g id="ytick_11"><g id="line2d_80"><g><use style="stroke:#000000;stroke-width:0.8;" x="391.267187" xlink:href="#m1b3fe9340e" y="36.484966"></use></g></g><g id="text_23"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:start;" transform="rotate(-0, 398.267187, 40.284184)" x="398.267187" y="40.284184">100%</text></g></g><g id="text_24"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-90, 438.454687, 98.582812)" x="438.454687" y="98.582812">Percentage of features interpretable</text></g></g><g id="LineCollection_1"><path clip-path="url(#p1682c8023d)" d="M 250.579922 165.469686 L 250.579922 115.5768 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path><path clip-path="url(#p1682c8023d)" d="M 271.114425 157.947463 L 271.114425 141.793401 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path><path clip-path="url(#p1682c8023d)" d="M 293.618253 153.528028 L 293.618253 142.148841 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path><path clip-path="url(#p1682c8023d)" d="M 314.152756 134.071552 L 314.152756 107.96058 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path><path clip-path="url(#p1682c8023d)" d="M 336.656585 96.84294 L 336.656585 59.032493 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path><path clip-path="url(#p1682c8023d)" d="M 357.191087 64.393362 L 357.191087 48.403723 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path><path clip-path="url(#p1682c8023d)" d="M 379.694916 71.266958 L 379.694916 43.968524 " style="fill:none;stroke:#000000;stroke-width:1.5;"></path></g><g id="line2d_81"><defs><path d="M -0.49999 0.49999 L 0.50001 0.49999 L 0.50001 -0.50001 L -0.49999 -0.50001 z " id="m97d41825aa"></path></defs><g clip-path="url(#p1682c8023d)"><use x="250.579922" xlink:href="#m97d41825aa" y="140.523243"></use><use x="271.114425" xlink:href="#m97d41825aa" y="149.870432"></use><use x="293.618253" xlink:href="#m97d41825aa" y="147.838435"></use><use x="314.152756" xlink:href="#m97d41825aa" y="121.016066"></use><use x="336.656585" xlink:href="#m97d41825aa" y="77.937717"></use><use x="357.191087" xlink:href="#m97d41825aa" y="56.398542"></use><use x="379.694916" xlink:href="#m97d41825aa" y="57.617741"></use></g></g><g id="patch_16"><path d="M 239.085369 166.532812 L 239.085369 30.632812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_17"><path d="M 391.267187 166.532812 L 391.267187 30.632812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_18"><path d="M 239.085369 166.532812 L 391.267187 166.532812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="patch_19"><path d="M 239.085369 30.632812 L 391.267187 30.632812 " style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"></path></g><g id="text_25"><text style="font-family:DejaVu Sans;font-size:12px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-0, 315.176278, 24.632812)" x="315.176278" y="24.632812">Interpretable features</text></g></g><g id="axes_3"><g id="matplotlib.axis_5"><g id="xtick_64"><g id="text_26"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:middle;" transform="rotate(-0, 56.467188, 181.13125)" x="56.467188" y="181.13125">0.0</text></g></g><g id="xtick_65"><g id="text_27"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:middle;" transform="rotate(-0, 123.427188, 181.13125)" x="123.427188" y="181.13125">0.2</text></g></g><g id="xtick_66"><g id="text_28"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:middle;" transform="rotate(-0, 190.387188, 181.13125)" x="190.387188" y="181.13125">0.4</text></g></g><g id="xtick_67"><g id="text_29"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:middle;" transform="rotate(-0, 257.347188, 181.13125)" x="257.347188" y="181.13125">0.6</text></g></g><g id="xtick_68"><g id="text_30"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:middle;" transform="rotate(-0, 324.307188, 181.13125)" x="324.307188" y="181.13125">0.8</text></g></g><g id="xtick_69"><g id="text_31"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:middle;" transform="rotate(-0, 391.267188, 181.13125)" x="391.267188" y="181.13125">1.0</text></g></g><g id="text_32"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;text-anchor:middle;" transform="rotate(-0, 223.867188, 194.809375)" x="223.867188" y="194.809375">Number of training levels</text></g></g><g id="matplotlib.axis_6"><g id="ytick_12"><g id="text_33"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:end;" transform="rotate(-0, 49.467188, 170.332031)" x="49.467188" y="170.332031">0.0</text></g></g><g id="ytick_13"><g id="text_34"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:end;" transform="rotate(-0, 49.467188, 143.152031)" x="49.467188" y="143.152031">0.2</text></g></g><g id="ytick_14"><g id="text_35"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:end;" transform="rotate(-0, 49.467188, 115.972031)" x="49.467188" y="115.972031">0.4</text></g></g><g id="ytick_15"><g id="text_36"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:end;" transform="rotate(-0, 49.467188, 88.792031)" x="49.467188" y="88.792031">0.6</text></g></g><g id="ytick_16"><g id="text_37"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:end;" transform="rotate(-0, 49.467188, 61.612031)" x="49.467188" y="61.612031">0.8</text></g></g><g id="ytick_17"><g id="text_38"><text style="font-family:DejaVu Sans;font-size:10px;font-style:normal;font-weight:normal;opacity:0;text-anchor:end;" transform="rotate(-0, 49.467188, 34.432031)" x="49.467188" y="34.432031">1.0</text></g></g></g></g></g><defs><clipPath id="pd74eaed00d"><rect height="135.9" width="152.181818" x="56.467188" y="30.632812"></rect></clipPath><clipPath id="p1682c8023d"><rect height="135.9" width="152.181818" x="239.085369" y="30.632812"></rect></clipPath></defs></svg>
      <figcaption>Comparison of models trained on different numbers of levels. Two models were trained for each number of levels, and two researchers independently evaluated how interpretable the features of each model were, without being shown the number of levels.<d-footnote>Our methodology had some flaws. Firstly, the researchers were not completely blind to the number of levels: for example, it is possible to infer something about the number of levels from the smoothness of graphs of the value function, since with fewer levels the model is better able to memorize the number of timesteps until the end of the level. Secondly, since evaluations are somewhat tedious, we stopped them once we thought the trend had become clear, introducing some selection bias. Therefore these results should be considered primarily illustrative.</d-footnote> Each model was tested on 10,000 train and 10,000 test levels sampled with replacement. Shaded areas in the left plot show the range of values over both models, though these are mostly too narrow to be visible. Error bars in the right plot show &plusmn;1 population standard deviation over all four model&ndash;researcher pairs.</figcaption>
    </figure>
    <p>
      Our results illustrate how diversity may lead to interpretable features via generalization, lending support to the diversity hypothesis. Nevertheless, we still consider the hypothesis to be highly unproven.
    </p>
    <h2 id="feature-visualization">Feature visualization</h2>
    <p>
      <a href="https://distill.pub/2017/feature-visualization/">Feature visualization</a> <d-cite key="attribution1,featurevis,featurevis1,featurevis2,featurevis3,featurevis4"></d-cite> answers questions about what certain parts of a network are looking for by generating examples. This can be done by applying gradient descent to the input image, starting from random noise, with the objective of activating a particular neuron or group of neurons. While this method works well for an image classifier trained on ImageNet <d-cite key="imagenet"></d-cite>, for our CoinRun model it yields only featureless clouds of color. Only for the first layer, which computes simple convolutions of the input, does the method produce comparable visualizations for the two models.
    </p>
    <figure style="grid-column: page;">
      <div style="display: table; margin: 0 auto;">
        <table id="feature-vis-traditional">
          <tr><th></th><th>ImageNet</th><th>CoinRun</th></tr>
          <tr>
            <td>
              First layer
            </td>
            <td>
              <img src="images/feature_vis_traditional/imagenet_first_0.png">
              <img src="images/feature_vis_traditional/imagenet_first_1.png">
              <img src="images/feature_vis_traditional/imagenet_first_2.png">
            </td>
            <td>
              <img src="images/feature_vis_traditional/coinrun_first_0.png">
              <img src="images/feature_vis_traditional/coinrun_first_1.png">
              <img src="images/feature_vis_traditional/coinrun_first_2.png">
            </td>
          </tr>
          <tr>
            <td style="width: 1%;">
              Intermediate layer
            </td>
            <td>
              <img src="images/feature_vis_traditional/imagenet_intermediate_0.png">
              <img src="images/feature_vis_traditional/imagenet_intermediate_1.png">
              <img src="images/feature_vis_traditional/imagenet_intermediate_2.png">
            </td>
            <td>
              <img src="images/feature_vis_traditional/coinrun_intermediate_0.png">
              <img src="images/feature_vis_traditional/coinrun_intermediate_1.png">
              <img src="images/feature_vis_traditional/coinrun_intermediate_2.png">
            </td>
          </tr>
        </table>
        <figcaption style="display: table-caption; caption-side: bottom;">Comparison of gradient-based feature visualization for CNNs trained on ImageNet (GoogLeNet <d-cite key="googlenet"></d-cite>) and on CoinRun (architecture described <a href="#architecture">below</a>). Each image was chosen to activate a neuron in the center, with the 3 images corresponding to the first 3 channels. Jittering was applied between optimization steps of up to 2 pixels for the first layer, and up to 8 pixels for the intermediate layer (mixed4a for ImageNet, <a href="#architecture">2b</a> for CoinRun).</figcaption>
      </div>
    </figure>
    <p>
      Gradient-based feature visualization has previously been shown to struggle with RL models trained on Atari games <d-cite key="atarimodelzoo,atarifeaturevis"></d-cite>. To try to get it to work for CoinRun, we varied the method in a number of ways. Nothing we tried had any noticeable effect on the quality of the visualizations.
    </p>
    <ul>
      <li><b>Transformation robustness.</b> This is the method of stochastically jittering, rotating and scaling the image between optimization steps, to search for examples that are robust to these transformations <d-cite key="featurevis"></d-cite>. We tried both increasing and decreasing the size of the jittering. Rotating and scaling are less appropriate for CoinRun, since the observations themselves are not invariant to these transformations.</li>
      <li id="extremal-colors"><b>Penalizing extremal colors.</b><d-footnote>By an "extremal" color we mean one of the 8 colors with maximal or minimal RGB values (black, white, red, green, blue, yellow, cyan and magenta).</d-footnote> Noticing that our visualizations tend to use extremal colors towards the middle, we tried including in the visualization objective an L2 penalty of various strengths on the activations of the first layer, which successfully reduced the size of the extremally-colored region but did not otherwise help.</li>
      <li><b>Alternative objectives.</b> We tried using an alternative optimization objective <d-cite key="featurevis"></d-cite>, such as the caricature objective.<d-footnote>The caricature objective is to maximize the dot product between the activations of the input image and the activations of a reference image. Caricatures are often an especially easy type of feature visualization to make work, and helpful for getting a first glance into what features a model has. They are demonstrated in <a href="https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/feature_inversion_caricatures.ipynb">this notebook</a>. A more detailed manuscript by its authors <d-cite key="caricatures"></d-cite> is forthcoming.</d-footnote> We also tried using dimensionality reduction, as described <a href="#dataset-examples">below</a>, to choose non-axis-aligned directions in activation space to maximize.
      <li><b>Low-level visual diversity.</b> In an attempt to broaden the distribution of images seen by the model, we retrained it on a version of the game with procedurally-generated sprites. We additionally tried adding noise to the images, both independent per-pixel noise and spatially-correlated noise. Finally, we experimented briefly with adversarial training <d-cite key="adversarialtraining"></d-cite>, though we did not pursue this line of inquiry very far.</li>
    </ul>
    <p id="feature-visualization-discussion">
      As shown <a href="#dataset-examples">below</a>, we were able to use dataset examples to identify a number of channels that pick out human-interpretable features. It is therefore striking how resistant gradient-based methods were to our efforts. We believe that this is because solving CoinRun does not ultimately require much visual ability. Even with our modifications, it is possible to solve the game using simple visual shortcuts, such as picking out certain small configurations of pixels. These shortcuts work well on the narrow distribution of images on which the model is trained, but behave unpredictably in the full space of images in which gradient-based optimization takes place.
    </p>
    <p>
      Our analysis here provides further insight into the <a href="#diversity-hypothesis">diversity hypothesis</a>. In support of the hypothesis, we have examples of features that are hard to interpret in the absence of diversity. But there is also evidence that the hypothesis may need to be refined. Firstly, it seems to be a lack of diversity at a low level of abstraction that harms our ability to interpret features at all levels of abstraction, which could be due to the fact that gradient-based feature visualization needs to back-propagate through earlier layers. Secondly, the failure of our efforts to increase low-level visual diversity suggests that diversity may need to be assessed in the context of the requirements of the task.
    </p>
    <h3 id="dataset-examples">Dataset example-based feature visualization</h3>
    <p>
      As an alternative to gradient-based feature visualization, we use dataset examples. This idea has a long history, and can be thought of as a heavily-regularized form of feature visualization <d-cite key="featurevis,featurevisadversarial"></d-cite>. In more detail, we sample a few thousand observations infrequently from the agent playing the game, and pass them through the model. We then apply a dimensionality reduction method known as non-negative matrix factorization (NMF) to the activation channels <d-cite key="buildingblocks"></d-cite>.<d-footnote>More precisely, we find a non-negative approximate low-rank factorization of the matrix obtained by flattening the spatial dimensions of the activations into the batch dimension. This matrix has one row per observation <i>per spatial position</i> and one column per channel: thus the dimensionality reduction does not use spatial information.</d-footnote> For each of the resulting channels (which correspond to weighted combinations of the original channels), we choose the observations and spatial positions with the strongest activation (with a limited number of examples per position, for diversity), and display a patch from the observation at that position.
    </p>
    <figure>
      <div id="feature-vis-dataset">
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_0.png"><div class="feature-vis-dataset-text">Short left-facing wall</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_1.png"><div class="feature-vis-dataset-text">Velocity info or left edge of screen</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_2.png"><div class="feature-vis-dataset-text">Long left-facing wall</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_3.png"><div class="feature-vis-dataset-text">Left end of platform</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_4.png"><div class="feature-vis-dataset-text">Right end of platform</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_5.png"><div class="feature-vis-dataset-text">Buzzsaw obstacle or platform</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_6.png"><div class="feature-vis-dataset-text">Coin</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_7.png"><div class="feature-vis-dataset-text">Top/right edge of screen</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_8.png"><div class="feature-vis-dataset-text">Left end of platform</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_9.png"><div class="feature-vis-dataset-text">Step</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_10.png"><div class="feature-vis-dataset-text">Agent or enemy moving right</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_11.png"><div class="feature-vis-dataset-text">Left edge of box</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_12.png"><div class="feature-vis-dataset-text">Right end of platform</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_13.png"><div class="feature-vis-dataset-text">Buzzsaw obstacle</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_14.png"><div class="feature-vis-dataset-text">Top left corner of box</div></div>
        <div class="feature-vis-dataset-item"><img src="images/feature_vis_dataset/layer_2b_feature_15.png"><div class="feature-vis-dataset-text">Left end of platform or bottom/right of screen?</div></div>
      </div>
      <figcaption>Dataset example-based feature visualizations for 16 NMF directions of layer <a href="#architecture">2b</a> of our CoinRun model. The grey-white checkerboard represents the edge of the screen. The labels are hand-composed.</figcaption>
    </figure>
    <p>
      Unlike gradient-based feature visualization, this method finds some meaning to the different directions in activation space. However, it may still fail to provide a complete picture for each direction, since it only shows a limited number of dataset examples, and with limited context.
    </p>
    <h3 id="feature-visualization-spatial">Spatially-aware feature visualization</h3>
    <p>
      CoinRun observations differ from natural images in that they are much less spatially invariant. For example, the agent always appears in the center, and the agent's velocity is always encoded in the top left. As a result, some features detect unrelated things at different spatial positions, such as reading the agent's velocity in the top left while detecting an unrelated object elsewhere. To account for this, we developed a spatially-aware version of dataset example-based feature visualization, in which we fix each spatial position in turn, and choose the observation with the strongest activation at that position (with a limited number of reuses of the same observation, for diversity). This creates a spatial correspondence between visualizations and observations.
    </p>
    <p>
      Here is such a visualization for a feature that responds strongly to coins. The white squares in the top left show that the feature also responds strongly to the horizontal velocity info when it is white, corresponding to the agent moving right at full speed.
    </p>
    <figure>
      <div id="feature-vis-spatial" class="striped">
        <img src="images/feature_vis_spatial.png">
      </div>
      <figcaption>Spatially-aware dataset example-based feature visualization for the coin-detecting NMF direction of layer <a href="#architecture">2b</a>. Transparency (revealing the diagonally-striped background) indicates a weak response, so the left half of the visualization is mostly transparent because coins never appear in the left half of observations.</figcaption>
    </figure>
    <h2 id="attribution">Attribution</h2>
    <p>
      Attribution <d-cite key="attribution1,attribution2,attribution3,gradcam,attribution4,attribution5,attribution6,integratedgradients"></d-cite> answers questions about the relationships between neurons. It is most commonly used to see how the input to a network affects a particular output <d-cite key="perturbationsaliency"></d-cite>, but it can also be applied to the activations of hidden layers <d-cite key="buildingblocks"></d-cite>. Although there are many approaches to attribution we could have used, we chose the method of integrated gradients <d-cite key="integratedgradients"></d-cite>. We explain in <a href="#integrated-gradients">Appendix B</a> how we applied this method a hidden layer, and how positive value function attribution can be thought of as "good news" and negative value function attribution can as "bad news".
    </p>
    <h3 id="dimensionality-reduction">Dimensionality reduction for attribution</h3>
    <p>
      We showed <a href="#dataset-examples">above</a> that a dimensionality reduction method known as non-negative matrix factorization (NMF) could be applied to the channels of activations to produce meaningful directions in activation space <d-cite key="buildingblocks"></d-cite>. We found that it is even more effective to apply NMF not to activations, but to <span style="font-style: italic;">value function attributions</span><d-footnote>As before, we obtain the NMF directions by sampling a few thousand observations infrequently from the agent playing the game, computing the attributions, flattening the spatial dimensions into the batch dimension, and applying NMF.</d-footnote> (working around the fact that NMF can only be applied to non-negative matrices<d-footnote>Our workaround is to separate out the positive and negative parts of the attributions and concatenate them along the batch dimension. We could also have concatenated them along the channel dimension.</d-footnote>). Both methods tend to produce NMF directions that are close to one-hot, and so can be thought of as picking out the most relevant channels. However, when reducing to a small number of dimensions, using attributions usually picks out more salient features, because attribution takes into account not just <span style="font-style: italic;">what neurons respond to</span> but also <span style="font-style: italic;">whether their response matters</span>.
    </p>
    <p>
      Following <d-cite key="buildingblocks"></d-cite>, after applying NMF to attributions, we visualize them by assigning a different color to each of the resulting channels. We overlay these visualizations over the observation <d-cite key="gradcam"></d-cite> and contextualize each channel using feature visualization <d-cite key="buildingblocks"></d-cite>, making use of <a href="#dataset-examples">dataset example-based feature visualization</a>. This gives a basic version of our interface, which allows us to see the effect of the main features at different spatial positions.
    </p>
    <figure style="grid-column: page;">
      <div id="attribution-demo" style="display: table; margin: 0 auto; width: 100%;">
        <table style="margin-bottom: 1em; width: 100%;">
          <tr><th>Observation</th><th>Positive attribution <span style="font-weight: normal;">(good news)</span></th><th>Negative attribution <span style="font-weight: normal;">(bad news)</span></th></tr>
          <tr>
            <td>
              <div class="attribution-outer">
                <div class="attribution-inner" style="z-index: 0;">
                  <div class="attribution-image" style="background-image: url('images/attribution/observation.png');"></div>
                </div>
              </div>
            </td>
            <td>
              <div class="attribution-outer">
                <div class="attribution-inner" style="z-index: 0;">
                  <div class="attribution-image grayscale-dark" style="background-image: url('images/attribution/observation.png');"></div>
                </div>
                <div class="attribution-inner" style="z-index: 1;">
                  <div id="attribution-overlay-pos" class="attribution-image" style="background-image: url('images/attribution/attribution_pos.png');"></div>
                </div>
              </div>
            </td>
            <td>
              <div class="attribution-outer">
                <div class="attribution-inner" style="z-index: 0;">
                  <div class="attribution-image grayscale-dark" style="background-image: url('images/attribution/observation.png');"></div>
                </div>
                <div class="attribution-inner" style="z-index: 1;">
                  <div id="attribution-overlay-neg" class="attribution-image" style="background-image: url('images/attribution/attribution_neg.png');"></div>
                </div>
              </div>
            </td>
          </tr>
        </table>
        <div style="text-align: center; margin-bottom: 0.5em;">
          <span style="font-weight: bold; border-bottom: 1px solid lightgray;">Legend <span style="font-weight: normal;">(hover to isolate)</span></span>
          <br>
          <div id="attribution-legend-item-0" class="attribution-legend-item">
            <div class="attribution-legend-outer">
              <div class="attribution-legend-dot" style="background-color: #ff0000;"></div>
              <div class="attribution-legend-inner">
                <div class="attribution-image" style="background-image: url('images/attribution/layer_2b_feature_0.png');"></div>
              </div>
              <div class="attribution-legend-label">Buzzsaw<br>obstacle</div>
            </div>
          </div>
          <div id="attribution-legend-item-1" class="attribution-legend-item">
            <div class="attribution-legend-outer">
              <div class="attribution-legend-dot" style="background-color: #ffe400;"></div>
              <div class="attribution-legend-inner">
                <div class="attribution-image" style="background-image: url('images/attribution/layer_2b_feature_1.png');"></div>
              </div>
              <div class="attribution-legend-label">Coin</div>
            </div>
          </div>
          <div id="attribution-legend-item-2" class="attribution-legend-item">
            <div class="attribution-legend-outer">
              <div class="attribution-legend-dot" style="background-color: #a1ff00;"></div>
              <div class="attribution-legend-inner">
                <div class="attribution-image" style="background-image: url('images/attribution/layer_2b_feature_2.png');"></div>
              </div>
              <div class="attribution-legend-label">Enemy<br>moving<br>left</div>
            </div>
          </div>
          <div id="attribution-legend-item-3" class="attribution-legend-item">
            <div class="attribution-legend-outer">
              <div class="attribution-legend-dot" style="background-color: #00ffff;"></div>
              <div class="attribution-legend-inner">
                <div class="attribution-image" style="background-image: url('images/attribution/layer_2b_feature_3.png');"></div>
              </div>
              <div class="attribution-legend-label">Agent<br>or enemy<br>moving right</div>
            </div>
          </div>
        </div>
        <figcaption style="display: table-caption; caption-side: bottom;">
          Value function attribution for a cherry-picked observation using layer <a href="#architecture">2b</a> of our CoinRun model, reduced to 4 channels using attribution-based NMF. The dataset example-based feature visualizations of these directions reveal more salient features than the visualizations of the first 4 activation-based NMF directions from the preceding section.
        </figcaption>
      </div>
    </figure>
    <p>
      For the <a href="#interface">full version</a> of our interface, we simply repeat this for an entire trajectory of the agent playing the game. We also incorporate video controls, a timeline view of compressed observations <d-cite key="rmo"></d-cite>, and additional information, such as model outputs and sampled actions. Together these allow the trajectory to be easily explored and understood.
    </p>
    <h3 id="attribution-discussion">Attribution discussion</h3>
    <p>
      Attributions for our CoinRun model have some interesting properties that would be unusual for an ImageNet model.
    </p>
    <ul>
      <li><b>Sparsity.</b> Attribution tends to be concentrated in a very small number of spatial positions and (post-NMF) channels. For example, in the figure above, the top 10 position&ndash;channel pairs account for more than 80% of the total absolute attribution. This might be explained by our <a href="#feature-visualization-discussion">earlier</a> hypothesis that the model identifies objects by picking out certain small configurations of pixels. Because of this sparsity, we smooth out attribution over nearby spatial positions for the full version of our interface, so that the amount of visual space taken up can be used to judge attribution strength. This trades off some spatial precision for more precision with magnitudes.</li>
      <li><b>Unexpected sign.</b> Value function attribution usually has the sign one would expect: positive for coins, negative for enemies, and so on. However, this is sometimes not the case. For example, in the figure above, the red channel that detects buzzsaw obstacles has both positive and negative attribution in two neighboring spatial positions towards the left. Our best guess is that this phenomenon is a result of statistical <a href="https://en.wikipedia.org/wiki/Multicollinearity">collinearity</a>, caused by certain correlations in the procedural level generation together with the agent's behavior. These could be visual, such as correlations between nearby pixels, or more abstract, such as both coins and long walls appearing at the end of every level. As a toy example, supposing the value function ought to increase by 2% when the end of the level becomes visible, the model could either increase the value function by 1% for coins and 1% for long walls, or by 3% for coins and &minus;1% for long walls, and the effect would be similar.</li>
      <li><b>Outlier frames.</b> When an unusual event causes the network to output extreme values, attribution can behave especially strangely. For example, in the <a id="bug-saw-link" href="#hallucinations">buzzsaw hallucination</a> frame, most features have a significant amount of both positive and negative attribution. We do not have a good explanation for this, but perhaps features are interacting in more complicated ways than usual. Moreover, in these cases there is often a significant component of the attribution lying outside the space spanned by the NMF directions, which we display as an additional "residual" feature. This could be because each frame is weighted equally when computing NMF, so outlier frames have little influence over the NMF directions.</li>
    </ul>
    <p>
      These considerations suggest that some care may be required when interpreting attributions.
    </p>
    <h2 id="questions">Questions for further research</h2>
    <h3>The <a href="#diversity-hypothesis">diversity hypothesis</a></h3>
    <ol>
      <li><b>Validity.</b> Does the diversity hypothesis hold in other contexts, both within and outside of reinforcement learning?</li>
      <li><b>Relationship to generalization.</b> What is the three-way relationship between diversity, interpretable features and generalization? Do non-interpretable features indicate that a model will fail to generalize in certain ways? Generalization refers implicitly to an underlying distribution &ndash; how should this distribution be chosen?<d-footnote>For example, to measure generalization for CoinRun models trained on a limited number of levels, we used the distribution over all possible procedurally-generated levels. However, to formalize the sense in which CoinRun is not diverse in its visual patterns or dynamics rules, one would need a distribution over levels from a wider class of games.</d-footnote></li>
      <li><b>Caveats.</b> How are interpretable features affected by other factors, such as the choice of task or algorithm, and how do these interact with diversity? Speculatively, do big enough models obtain interpretable features via the double descent phenomenon <d-cite key="doubledescent"></d-cite>, even in the absence of diversity?</li>
      <li><b>Quantification.</b> Can we quantitatively predict how much diversity is needed for interpretable features, perhaps using generalization metrics? Can we be precise about what is meant by an "interpretable feature" and a "level of abstraction"?</li>
    </ol>
    <h3>Interpretability in the absence of diversity</h3>
    <ol>
      <li><b>Pervasiveness of non-diverse features.</b> Do "non-diverse features", by which we mean the hard-to-interpret features that tend to arise in the absence of diversity, remain when diversity is present? Is there a connection between these non-diverse features and the "non-robust features" that have been posited to explain adversarial examples <d-cite key="advexfeatures,advexfeaturesdiscussion"></d-cite>?</li>
      <li><b>Coping with non-diverse levels of abstraction.</b> Are there levels of abstraction at which even broad distributions like ImageNet remain non-diverse, and how can we best interpret models at these levels of abstraction?</li>
      <li><b>Gradient-based feature visualization.</b> Why does gradient-based feature visualization <a href="#feature-visualization">break down</a> in the absence of diversity, and can it be made to work using transformation robustness, regularization, data augmentation, adversarial training, or other techniques? What property of the optimization leads to the clouds of <a href="#extremal-colors">extremal colors</a>?</li>
      <li><b>Trustworthiness of dataset examples and attribution.</b> How reliable and trustworthy can we make very heavily-regularized versions of feature visualization, such as those based on <a href="#dataset-examples">dataset examples</a>?<d-footnote>Heavily-regularized feature visualization may be untrustworthy by failing to separate the things causing certain behavior from the things that merely correlate with those causes <d-cite key="featurevis"></d-cite>.</d-footnote> What explains the <a href="#attribution-discussion">strange behavior</a> of attribution, and how trustworthy is it?</li>
    </ol>
    <h3>Interpretability in the RL framework</h3>
    <ol>
      <li><b>Non-visual and abstract features.</b> What are the best methods for interpreting models with non-visual inputs? Even vision models may also have interpretable abstract features, such as relationships between objects or anticipated events: will any method of generating examples be enough to understand these, or do we need an entirely new approach? For models with memory, how can we interpret their hidden states <d-cite key="capturetheflag,rubik,dota"></d-cite>?</li>
      <li><b>Improving reliability.</b> How can we best identify, understand and correct rare <a href="#dissecting-failure">failures</a> and <a href="#hallucinations">other errors</a> in RL models? Can we actually improve models by <a href="#model-editing">model editing</a>, rather than merely degrading them?</li>
      <li><b>Modifying training.</b> In what ways can we train RL models to make them more interpretable without a significant performance cost, such as by altering architectures or adding auxiliary predictive losses?</li>
      <li><b>Leveraging the environment.</b> How can we enrich interfaces using RL-specific data, such as trajectories of agent&ndash;environment interaction, state distributions, and advantage estimates? What are the benefits of incorporating user&ndash;environment interaction, such as for exploring counterfactuals?</li>
    </ol>
    <h3 id="questions-discussion">What we would like to see from further research and why</h3>
    <p>
      We are motivated to study interpretability for RL for two reasons.
    </p>
    <ul>
      <li><b>To be able to interpret RL models.</b> RL can be applied to an enormous variety of tasks, and seems likely to be a part of increasingly influential AI systems. It is therefore important to be able to scrutinize RL models and to understand how they might fail. This may also benefit RL research through an improved understanding of the pitfalls of different algorithms and environments.</li>
      <li><b>As a testbed for interpretability techniques.</b> RL models pose a number of distinctive challenges for interpretability techniques. In particular, environments like CoinRun straddle the boundary between memorization and generalization, making them useful for studying the <a href="#diversity-hypothesis">diversity hypothesis</a> and related ideas.</li>
    </ul>
    <p>
      We think that large neural networks are currently the most likely type of model to be used in highly capable and influential AI systems in the future. Contrary to the traditional perception of neural networks as black boxes, we think that there is a fighting chance that we will be able to clearly and thoroughly understand the behavior even of very large networks. We are therefore most excited by neural network interpretability research that scores highly according to the following criteria.
    </p>
    <ul>
      <li><b>Scalability.</b> The takeaways of the research should have some chance of scaling to harder problems and larger networks. If the techniques themselves do not scale, they should at least reveal some relevant insight that might.</li>
      <li><b>Trustworthiness.</b> Explanations should be faithful to the model. Even if they do not tell the full story, they should at least not be biased in some fatal way (such as by using an approval-based objective that leads to bad explanations that sound good, or by depending on another model that badly distorts information).</li>
      <li><b>Exhaustiveness.</b> This may turn out to be impossible at scale, but we should strive for techniques that explain every essential feature of our models. If there are theoretical limits to exhaustiveness, we should try to understand these.</li>
      <li><b>Low cost.</b> Our techniques should not be significantly more computationally expensive than training the model. We hope that we will not need to train models differently for them to be interpretable, but if we do, we should try to minimize both the computational expense and any performance cost, so that interpretable models are not disincentivized from being used in practice.</li>
    </ul>
    <p>
      Our proposed questions reflect this perspective. One of the reasons we emphasize diversity relates to exhaustiveness. If "non-diverse features" remain when diversity is present, then our current techniques are not exhaustive and could end up missing important features of more capable models. Developing tools to understand non-diverse features may shed light on whether this is likely to be a problem.
    </p>
    <p>
      We think there may be significant mileage in simply applying existing interpretability techniques, with attention to detail, to more models. Indeed, this was the mindset with which we initially approached this project. If the diversity hypothesis is correct, then this may become easier as we train our models to perform more complex tasks. Like early biologists encountering a new species, there may be a lot we can glean from taking a magnifying glass to the creatures in front of us.
    </p>
    <h2 id="supplementary-material">Supplementary material</h2>
    <ul>
      <li><b>Code.</b> Utilities for computing feature visualization, attribution and dimensionality reduction for our models can be found in <code>lucid.scratch.rl_util</code>, a submodule of <a href="https://github.com/tensorflow/lucid">Lucid</a>. We demonstrate these in a <a href="https://colab.research.google.com/github/tensorflow/lucid/blob/master/notebooks/misc/rl_util.ipynb"><img src="images/colab.svg"> notebook</a>.</li>
      <li><b>Model weights.</b> The weights of our model are available for download, along with those of a number of other models, including the models trained on different numbers of levels, the edited models, and models trained on all 16 of the Procgen Benchmark <d-cite key="procgen"></d-cite> games. These are indexed <a href="https://openaipublic.blob.core.windows.net/rl-clarity/attribution/models/index.html">here</a>.</li>
      <li><b>More interfaces.</b> We generated an expanded version of our interface for every convolutional layer in our model, which can be found <a href="https://openaipublic.blob.core.windows.net/rl-clarity/attribution/demo/interface.html">here</a>. We also generated similar interfaces for each of our other models, which are indexed <a href="https://openaipublic.blob.core.windows.net/rl-clarity/attribution/index.html">here</a>.</li>
      <li><b>Interface code.</b> The code used to generate the expanded version of our interface can be found <a href="https://github.com/openai/understanding-rl-vision">here</a>.</li>
    </ul>

  </d-article>

  <d-appendix id="appendix">
    <h3 id="model-editing-method">Appendix A: Model editing method</h3>
    <p>
      Here we explain our method for <a href="#model-editing">editing the model</a> to make the agent blind to certain features.
    </p>
    <p>
      The features in our interface correspond to directions in activation space obtained by applying <a href="#dimensionality-reduction">attribution-based NMF</a> to layer <a href="#architecture">2b</a> of our model. To blind the agent to a feature, we edit the weights to make them project out the corresponding NMF direction.
    </p>
    <p>
      More precisely, let <d-math>\mathbf v</d-math> be the NMF direction corresponding to the feature we wish to blind the model to. This is a vector of length <d-math>c</d-math>, the number of channels in activation space. Using this we construct the <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">orthogonal projection</a> matrix <d-math>P:=I-\frac 1{\|\mathbf v\|^2}\mathbf v\mathbf v^{\mathsf T}</d-math>, which projects out the direction of <d-math>\mathbf v</d-math> from activation vectors. We then take the convolutional kernel of the following layer, which has shape <d-math>\text{height}\times\text{width}\times c\times d</d-math>, where <d-math>d</d-math> is the number of output channels. Broadcasting across the height and width dimensions, we left-multiply each <d-math>c\times d</d-math> matrix in the kernel by <d-math>P</d-math>. The effect of the new kernel is to project out the direction of <d-math>\mathbf v</d-math> from activations before applying the original kernel.
    </p>
    <p>
      As it turned out, the NMF directions were close to one-hot, so this procedure is approximately equivalent to zeroing out the slice of the kernel corresponding to a particular in-channel.
    </p>
    <h3 id="integrated-gradients">Appendix B: Integrated gradients for a hidden layer</h3>
    <p>
      Here we explain the application of integrated gradients <d-cite key="integratedgradients"></d-cite> to a hidden layer for the purpose of <a href="#attribution">attribution</a>. This method can be applied to any of the network's outputs, but we focus here on the value function. Recall that this is the model's estimate of the time-discounted probability that the agent will successfully complete the level.
    </p>
    <div>
      <div style="float: right;">
        <figure style="width: 300px; margin: 0.5em 0.5em 1em 1em;">
          <svg height="160px" width="300px">
            <defs>
              <marker id="arrow-factorization" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto" markerUnits="strokeWidth">
                <path d="M0,0 L0,6 L9,3 z" fill="#000000" />
              </marker>
            </defs>
            <line x1="100" y1="30" x2="210" y2="30" stroke="#000000" stroke-width="1" marker-end="url(#arrow-factorization)"></line>
            <line x1="80" y1="50" x2="110" y2="100" stroke="#000000" stroke-width="1" marker-end="url(#arrow-factorization)"></line>
            <line x1="190" y1="100" x2="220" y2="50" stroke="#000000" stroke-width="1" marker-end="url(#arrow-factorization)"></line>
            <text x="50" y="20" text-anchor="middle">observation</text>
            <text x="50" y="40" text-anchor="middle">space</text>
            <text x="230" y="35" text-anchor="middle" class="katex"><tspan class="mathbb">R</tspan></text>
            <text x="150" y="120" text-anchor="middle">activation</text>
            <text x="150" y="140" text-anchor="middle">space</text>
            <text x="155" y="20" text-anchor="middle" class="katex"><tspan class="mathit">V</tspan></text>
            <text x="80" y="90" text-anchor="middle" class="katex"><tspan class="mathbf">A</tspan></text>
            <text x="220" y="90" text-anchor="middle" class="katex"><tspan class="mathit">F</tspan></text>
          </svg>
          <figcaption>The <a href="https://en.wikipedia.org/wiki/Commutative_diagram">diagram</a> defining <d-math>F</d-math>, whose gradient we take.</figcaption>
        </figure>
      </div>
      <p>
        Let <d-math>V:\mathbb R^{64\times 64\times 3}\to\mathbb R</d-math> be the value function computed by our network, which accepts a 64x64 RGB observation. Given any layer in the network, we may write <d-math>V</d-math> as <d-math>V\left(\mathbf x\right)=F\left(\mathbf A\left(\mathbf x\right)\right)</d-math>, where <d-math>\mathbf A</d-math> computes the layer's activations. Given an observation <d-math>\mathbf x</d-math>, a simple method of attribution is to compute <d-math>\nabla_{\mathbf a}F\left(\mathbf a\right)\odot\mathbf a</d-math>, where <d-math>\mathbf a=\mathbf A\left(\mathbf x\right)</d-math> and <d-math>\odot</d-math> denotes the pointwise product. This tells us the sensitivity of the value function to each activation, multiplied by the strength of that activation. However, it uses the sensitivity of the value function at the activation itself, which does not account for the fact that this sensitivity may change as the activation is increased from zero.
      </p>
    </div>
    <div>
      <div style="float: right;">
        <figure style="width: 300px; margin: 0.5em 0.5em 1em 1em;">
          <img src="images/integrated_gradient.png">
          <figcaption>Viewing <d-math>F</d-math> as the height of a surface, represented here using the background gradient, the integrated gradient of <d-math>F</d-math> measures the elevation gained while traveling in each direction, and sums to the total elevation gain <d-cite key="attributionpaths"></d-cite>.</figcaption>
        </figure>
      </div>
      <p>
        To account for this, the integrated gradients method instead chooses a path <d-math>\mathcal P</d-math> in activation space from some starting point <d-math>\mathbf a_0</d-math> to the ending point <d-math>\mathbf a_1:=\mathbf A\left(\mathbf x\right)</d-math>. We then compute the integrated gradient of <d-math>F</d-math> along <d-math>\mathcal P</d-math>, which is defined as the path integral <d-math block="">\int_{\mathcal P}\nabla_{\mathbf a}F\left(\mathbf a\right)\odot\mathrm d\mathbf a.</d-math> Note the use of the pointwise product rather than the usual dot product here, which makes the integral vector-valued. By the <a href="https://en.wikipedia.org/wiki/Gradient_theorem">fundamental theorem of calculus for line integrals</a>, when the components of the vector produced by this integral are summed, the result depends only on the endpoints <d-math>\mathbf a_0</d-math> and <d-math>\mathbf a_1</d-math>, equaling <d-math>F\left(\mathbf a_1\right)-F\left(\mathbf a_0\right)</d-math>. Thus the components of this vector provide a true decomposition of this difference, "attributing" it across the activations.
      </p>
      <p style="margin-bottom: 0;">
        For our purposes, we take <d-math>\mathcal P</d-math> to be the straight line from <d-math>\mathbf 0</d-math> to <d-math>\mathbf A\left(\mathbf x\right)</d-math>.<d-footnote>In theory, we could choose any point in activation space as the starting point of our path, but in practice, <d-math>\mathbf 0</d-math> tends to be a good baseline against which to compare other activations, with <d-math>F\left(\mathbf 0\right)</d-math> being on the same order as the average value function. Sundararajan, Taly and Yan <d-cite key="integratedgradients"></d-cite> discuss the choice of this baseline in more depth.</d-footnote> In other words, given an observation <d-math>\mathbf x</d-math>, we define the value function attribution as<d-footnote>In practice, we numerically approximate the integral by evaluating the integrand at <d-math>\alpha=0.1,0.2,\ldots,1</d-math>.</d-footnote>
      </p>
      <d-math block="" style="margin: 0 auto;">
        \int_{\alpha=0}^1\nabla_{\mathbf a}F\left(\alpha\mathbf A\left(\mathbf x\right)\right)\mathrm d\alpha\odot\mathbf A\left(\mathbf x\right).
      </d-math>
      <p>
        This has the same dimensions as <d-math>\mathbf A\left(\mathbf x\right)</d-math>, and its components sum to <d-math>V\left(\mathbf x\right)-F\left(\mathbf 0\right)</d-math>. So for a convolutional layer, this method allows us to attribute the value function (in excess of the baseline <d-math>F\left(\mathbf 0\right)</d-math>) across the horizontal, vertical and channel dimensions of activation space. Positive value function attribution can be thought of as "good news", components that cause the agent to think it is more likely to collect the coin at the end of the level. Similarly, negative value function attribution can be thought of as "bad news".
      </p>
    </div>
    <h3 id="architecture">Appendix C: Architecture</h3>
    <p>
      Our architecture consists of the following layers in the order given, together with ReLU activations for all except the final layer.
    </p>
    <ul class="architecture-list">
      <li>7x7 convolutional layer with 16 channels (layer 1a)</li>
      <li>2x2 L2 pooling layer</li>
      <li>5x5 convolutional layer with 32 channels (layer 2a)</li>
      <li>5x5 convolutional layer with 32 channels (layer 2b)</li>
      <li>2x2 L2 pooling layer</li>
      <li>5x5 convolutional layer with 32 channels (layer 3a)</li>
      <li>2x2 L2 pooling layer</li>
      <li>5x5 convolutional layer with 32 channels (layer 4a)</li>
      <li>2x2 L2 pooling layer</li>
      <li>256-unit dense layer</li>
      <li>512-unit dense layer</li>
      <li>10-unit dense layer (1 unit for the value function, 9 units for the policy logits)</li>
    </ul>
    <p>
      We designed this architecture by starting with the architecture from IMPALA <d-cite key="impala"></d-cite>, and making the following modifications in an attempt to aid interpretability without noticeably sacrificing performance.
    </p>
    <ul class="architecture-list">
      <li>We used fewer convolutional layers and more dense layers, to allow for more non-visual processing.</li>
      <li>We removed the residual connections, so that the flow of information passes through every layer.</li>
      <li>We made the pool size equal to the pool stride, to avoid gradient gridding.</li>
      <li>We used L2 pooling instead of max pooling, for more continuous gradients.</li>
    </ul>
    <p>
      The choice that seemed to make the most difference was using 5 rather than 12 convolutional layers, resulting in the object-identifying features (which were the most interpretable, as discussed <a href="#diversity-hypothesis">above</a>) being concentrated in a single layer (layer 2b), rather than being spread over multiple layers and mixed in with less interpretable features.
    </p>
    <h3>Acknowledgments</h3>
    <p>
      We would like to thank our reviewers Jonathan Uesato, Joel Lehman and one anonymous reviewer for their detailed and thoughtful feedback. We would also like to thank Karl Cobbe, Daniel Filan, Sam Greydanus, Christopher Hesse, Jacob Jackson, Michael Littman, Ben Millwood, Konstantinos Mitsopoulos, Mira Murati, Jorge Orbay, Alex Ray, Ludwig Schubert, John Schulman, Ilya Sutskever, Nevan Wichers, Liang Zhang and Daniel Ziegler for research discussions, feedback, follow-up work, help and support that have greatly benefited this project.
    </p>

    <h3>Author contributions</h3>
    <p>
      <b>Jacob Hilton</b> was the primary contributor.
    </p>
    <p>
      <b>Nick Cammarata</b> developed the model editing methodology and suggested applying it to CoinRun models.
    </p>
    <p>
      <b>Shan Carter</b> (while working at OpenAI) advised on interface design throughout the project, and worked on many of the diagrams in the article.
    </p>
    <p>
      <b>Gabriel Goh</b> provided evaluations of feature interpretability for the section <a href="#interpretability-and-generalization">Interpretability and generalization</a>.
    </p>
    <p>
      <b>Chris Olah</b> guided the direction of the project, performing initial exploratory research on the models, coming up with many of the research ideas, and helping to construct the article's narrative.
    </p>

    <h3>Discussion and Review</h3>
    <p>
      <a href="https://github.com/distillpub/post--understanding-rl-vision/issues/6">Review 1 - Anonymous</a><br>
      <a href="https://github.com/distillpub/post--understanding-rl-vision/issues/7">Review 2 - Jonathan Uesato</a><br>
      <a href="https://github.com/distillpub/post--understanding-rl-vision/issues/8">Review 3 - Joel Lehman</a><br>
    </p>

    <d-footnote-list></d-footnote-list>
    <d-citation-list></d-citation-list>
  </d-appendix>

  <d-bibliography src="bibliography.bib"></d-bibliography>

<script type="text/javascript" src="index.bundle.js"></script></body>
